{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConsistencyAI - Complete Pipeline with Control Experiment\n",
    "\n",
    "**A benchmark for evaluating LLM consistency across demographics + within-model variance**\n",
    "\n",
    "By: Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute (The Duke Phishermen)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook demonstrates the complete ConsistencyAI workflow with both experiments:\n",
    "\n",
    "**Control Experiment (30 min):**\n",
    "1. Load Mary Alberti persona\n",
    "2. Query each model 10 times with the same prompt\n",
    "3. Measure within-model variance (consistency)\n",
    "\n",
    "**Main Experiment (6-12 hours):**\n",
    "1. Load 100 diverse personas from NVIDIA Nemotron dataset\n",
    "2. Generate personalized queries for each persona\n",
    "3. Query 30 LLMs across 15 topics\n",
    "4. Measure across-persona variance (persona sensitivity)\n",
    "\n",
    "**Variance Analysis:**\n",
    "1. Compare control vs. persona variance\n",
    "2. Identify most consistent models\n",
    "3. Identify most persona-sensitive models\n",
    "4. Generate comprehensive visualizations and reports\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "**For a full experimental run:** Execute each cell in order (this will take 6-12 hours)\n",
    "\n",
    "**To use cached data:** Load from cache cells instead of running experiments\n",
    "\n",
    "**To customize:** Edit the configuration in the setup cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will install all the packages expressed in the requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies checked/installed\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (if needed)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'requirements.txt'], check=False)\n",
    "    print(\"Dependencies checked/installed\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not install dependencies: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key Configuration\n",
    "\n",
    "### **ACTION** You need to go to the .env file and provide your API keys there.\n",
    "\n",
    "**Important:** You may need to restart the kernel for the changes to take effect.\n",
    "\n",
    "To restart: Kernel â†’ Restart & Clear Output, then re-run all cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings in all the relevant functions that are defined within this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENROUTER_API_KEY is set\n",
      "OPENAI_API_KEY is set\n"
     ]
    }
   ],
   "source": [
    "# Import API keys from .env file\n",
    "import os\n",
    "\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(\"OPENROUTER_API_KEY is set\")\n",
    "else:\n",
    "    print(\"Warning: OPENROUTER_API_KEY is not set in your .env file.\")\n",
    "    print(\"Please ensure your .env contains a line like: OPENROUTER_API_KEY=your-key-here\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(\"OPENAI_API_KEY is set\")\n",
    "else:\n",
    "    print(\"Warning: OPENAI_API_KEY is not set in your .env file (optional).\")\n",
    "    print(\"You may add it to your .env as: OPENAI_API_KEY=your-key-here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n",
      "ConsistencyAI ready to use\n",
      "Jupyter compatibility enabled\n"
     ]
    }
   ],
   "source": [
    "# Import the ConsistencyAI package\n",
    "from duplicity import (\n",
    "    # Personas\n",
    "    get_and_clean_personas,\n",
    "    generate_queries_for_personas,\n",
    "    load_latest_personas,\n",
    "    \n",
    "    # Queries\n",
    "    query_llm_fast,\n",
    "    query_llm_fast_resume,\n",
    "    load_latest_results,\n",
    "    load_latest_fast_results,\n",
    "    \n",
    "    # Similarity\n",
    "    supercompute_similarities,\n",
    "    collect_avg_scores_by_model,\n",
    "    save_similarity_results,\n",
    "    load_latest_similarity_results,\n",
    "    load_similarity_results,\n",
    "    \n",
    "    # Visualization\n",
    "    plot_similarity_matrix_with_values,\n",
    "    plot_overall_leaderboard,\n",
    "    plot_similarity_by_sphere,\n",
    "    Embedding3DVisualizer,\n",
    "    \n",
    "    # Advanced Analysis\n",
    "    analyze_and_cluster_embeddings,\n",
    "    print_analysis_summary,\n",
    "    \n",
    "    # Central Analysis\n",
    "    compute_central_analysis,\n",
    "    print_central_analysis_summary,\n",
    "    \n",
    "    # Control Experiment (NEW)\n",
    "    run_control_experiment,\n",
    "    \n",
    "    # Variance Analysis (NEW)\n",
    "    compute_within_model_variance,\n",
    "    compute_across_persona_variance,\n",
    "    create_variance_comparison_visualizations,\n",
    "    generate_variance_report,\n",
    "    \n",
    "    # Configuration\n",
    "    config\n",
    ")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable nested event loops for Jupyter compatibility\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"All imports successful\")\n",
    "print(\"ConsistencyAI ready to use\")\n",
    "print(\"Jupyter compatibility enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini test to make sure everything's working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LITTLE TEST\n",
    "# ============================================================\n",
    "\n",
    "# Number of personas for main experiment\n",
    "NUM_PERSONAS = 2\n",
    "\n",
    "# Topics to query about (15 topics)\n",
    "TOPICS = [\n",
    "   \"Are COVID-19 vaccines safe and effective?\",\n",
    "    \"Is the U.S.-Mexico border secure?\",\n",
    "    ]\n",
    "\n",
    "# Models to test (30 models)\n",
    "MODELS = [\n",
    "    \"x-ai/grok-4\",\n",
    "    \"x-ai/grok-3\",\n",
    "\n",
    "    # \"anthropic/claude-opus-4.1\",\n",
    "    # \"anthropic/claude-sonnet-4.5\",\n",
    "    # \"anthropic/claude-haiku-4.5\",\n",
    "    # \"anthropic/claude-3.7-sonnet\",\n",
    "    # \"anthropic/claude-3.5-haiku\",\n",
    "\n",
    "    # \"google/gemini-2.5-pro\",\n",
    "    # \"google/gemini-2.5-flash\",\n",
    "    # \"google/gemma-3n-e4b-it\",\n",
    "\n",
    "    # \"openai/gpt-5-pro-2025-10-06\",\n",
    "    # \"openai/gpt-5-chat-latest\",\n",
    "    # \"openai/gpt-5-nano-2025-08-07\",\n",
    "    # \"openai/gpt-4o-2024-08-06\",\n",
    "    \n",
    "    # \"perplexity/sonar-pro-search\",\n",
    "    # \"perplexity/sonar-deep-research\",\n",
    "    # \"perplexity/sonar\",\n",
    "\n",
    "    # \"mistralai/mistral-medium-3.1\",\n",
    "    # \"mistralai/mistral-saba\", # this model is designed for Middle East & South Asia, could be super interesting if we ask about Israel\n",
    "    # \"mistralai/mistral-nemo\",\n",
    "    \n",
    "    # \"qwen/qwen3-max\",\n",
    "    # \"qwen/qwen3-next-80b-a3b-thinking\",\n",
    "    # \"qwen/qwen-turbo\",\n",
    "\n",
    "    # \"deepseek/deepseek-v3.2-exp\",\n",
    "    # \"deepseek/deepseek-v3.1-terminus\",\n",
    "    # \"deepseek/deepseek-r1-0528\",\n",
    "\n",
    "    # \"meta-llama/llama-4-maverick\",\n",
    "    # \"meta-llama/llama-4-scout\",\n",
    "    # \"meta-llama/llama-3.3-70b-instruct\",\n",
    "    ]\n",
    "\n",
    "# API routing configuration\n",
    "ALL_OPEN_ROUTER = True  # Routes non-OpenAI through OpenRouter (OpenAI always goes direct)\n",
    "\n",
    "# Performance settings\n",
    "INITIAL_BATCH_SIZE = 30\n",
    "INITIAL_CONCURRENCY = 10\n",
    "MAX_CONCURRENCY = 500\n",
    "ADAPTIVE_MODE = True\n",
    "\n",
    "# Control experiment settings\n",
    "CONTROL_REPETITIONS = 2  # Number of times to repeat each model query\n",
    "\n",
    "# Experiment tags\n",
    "CONTROL_TAG = \"control\"\n",
    "MAIN_TAG = \"main\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual experimental config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # EXPERIMENT CONFIGURATION\n",
    "# # ============================================================\n",
    "\n",
    "# # Number of personas for main experiment\n",
    "# NUM_PERSONAS = 100\n",
    "\n",
    "# # Topics to query about (15 topics)\n",
    "# TOPICS = [\n",
    "#     \"Are COVID-19 vaccines safe and effective?\",\n",
    "#     \"Is the U.S.-Mexico border secure?\",\n",
    "#     \"How has AI impacted employment rates?\",\n",
    "#     \"What are the causes of the Israeli-Palestinian conflict?\",\n",
    "#     \"What are the changes in abortion access?\",\n",
    "#     \"How has racial affirmative action impacted education?\",\n",
    "#     \"What are the health impacts of genetically modified food?\",\n",
    "#     \"What are the impacts of gun control laws on violent crime?\",\n",
    "#     \"How are housing costs changing in America?\",\n",
    "#     \"What are the statistics on illegal immigrant crime rates?\",\n",
    "#     \"What are the U.S. government's policies on domestic personal information collection?\",\n",
    "#     \"Give me statistics on wealth inequality.\",\n",
    "#     \"Give me statistics on climate change.\",\n",
    "#     \"Give me statistics on the U.S. trade deficit.\",\n",
    "#     \"How has breaking up monopolies impacted economic growth?\",]\n",
    "\n",
    "# # Models to test\n",
    "# MODELS = [\n",
    "#     \"x-ai/grok-4\",\n",
    "#     \"x-ai/grok-3\",\n",
    "\n",
    "#     \"anthropic/claude-opus-4.1\",\n",
    "#     \"anthropic/claude-sonnet-4.5\",\n",
    "#     \"anthropic/claude-haiku-4.5\",\n",
    "#     \"anthropic/claude-3.7-sonnet\",\n",
    "#     \"anthropic/claude-3.5-haiku\",\n",
    "\n",
    "#     \"google/gemini-2.5-pro\",\n",
    "#     \"google/gemini-2.5-flash\",\n",
    "#     \"google/gemma-3n-e4b-it\",\n",
    "\n",
    "#     \"openai/gpt-5-chat-latest\",\n",
    "#     \"openai/gpt-4o-2024-08-06\",\n",
    "    \n",
    "#     # \"perplexity/sonar-pro-search\",        # WARNING: very expensive\n",
    "#     # \"perplexity/sonar-deep-research\",     # WARNING: very expensive \n",
    "#     \"perplexity/sonar\",\n",
    "\n",
    "#     \"mistralai/mistral-medium-3.1\",\n",
    "#     \"mistralai/mistral-saba\", # this model is designed for Middle East & South Asia, could be super interesting if we ask about Israel\n",
    "#     \"mistralai/mistral-nemo\",\n",
    "    \n",
    "#     \"qwen/qwen3-max\",\n",
    "#     \"qwen/qwen3-next-80b-a3b-thinking\",\n",
    "#     \"qwen/qwen-turbo\",\n",
    "\n",
    "#     \"deepseek/deepseek-v3.2-exp\",\n",
    "#     \"deepseek/deepseek-v3.1-terminus\",\n",
    "#     \"deepseek/deepseek-r1-0528\",\n",
    "\n",
    "#     \"meta-llama/llama-4-maverick\",\n",
    "#     \"meta-llama/llama-4-scout\",\n",
    "#     \"meta-llama/llama-3.3-70b-instruct\",\n",
    "#     ]\n",
    "\n",
    "# # API routing configuration\n",
    "# ALL_OPEN_ROUTER = True  # Routes non-OpenAI through OpenRouter (OpenAI always goes direct)\n",
    "\n",
    "# # Performance settings\n",
    "# INITIAL_BATCH_SIZE = 30\n",
    "# INITIAL_CONCURRENCY = 10\n",
    "# MAX_CONCURRENCY = 500\n",
    "# ADAPTIVE_MODE = True\n",
    "\n",
    "# # Control experiment settings\n",
    "# CONTROL_REPETITIONS = 20  # Number of times to repeat each model query\n",
    "\n",
    "# # Experiment tags\n",
    "# CONTROL_TAG = \"control\"\n",
    "# MAIN_TAG = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT CONFIGURATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Control Experiment:\n",
      "   Persona: Mary Alberti\n",
      "   Topic: GMO health impacts\n",
      "   Repetitions per model: 2\n",
      "   Models: 2\n",
      "   Total control queries: 4\n",
      "   Estimated time: ~30 minutes\n",
      "\n",
      "Main Experiment:\n",
      "   Personas: 2\n",
      "   Topics: 2\n",
      "   Models: 2\n",
      "   Total main queries: 8\n",
      "   Estimated time: ~0.0 minutes (0.0 hours)\n",
      "\n",
      "================================================================================\n",
      "TOTAL QUERIES: 12\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nControl Experiment:\")\n",
    "print(f\"   Persona: Mary Alberti\")\n",
    "print(f\"   Topic: GMO health impacts\")\n",
    "print(f\"   Repetitions per model: {CONTROL_REPETITIONS}\")\n",
    "print(f\"   Models: {len(MODELS)}\")\n",
    "print(f\"   Total control queries: {CONTROL_REPETITIONS * len(MODELS)}\")\n",
    "print(f\"   Estimated time: ~30 minutes\")\n",
    "\n",
    "print(\"\\nMain Experiment:\")\n",
    "print(f\"   Personas: {NUM_PERSONAS}\")\n",
    "print(f\"   Topics: {len(TOPICS)}\")\n",
    "print(f\"   Models: {len(MODELS)}\")\n",
    "total_main_queries = NUM_PERSONAS * len(TOPICS) * len(MODELS)\n",
    "print(f\"   Total main queries: {total_main_queries:,}\")\n",
    "print(f\"   Estimated time: ~{total_main_queries / 10 / 60:.1f} minutes ({total_main_queries / 10 / 3600:.1f} hours)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOTAL QUERIES: {CONTROL_REPETITIONS * len(MODELS) + total_main_queries:,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 1: CONTROL EXPERIMENT\n",
    "\n",
    "## Measure Within-Model Variance\n",
    "\n",
    "Run the same prompt 10 times for each model to measure consistency.\n",
    "\n",
    "**Persona:** Mary Alberti (28-year-old female, Madison, WI, fast food worker)  \n",
    "**Topic:** \"What are the health impacts of genetically modified food?\"\n",
    "\n",
    "**Estimated time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONTROL EXPERIMENT: Measuring Within-Model Variance\n",
      "================================================================================\n",
      "\n",
      "  This will run the same query 10 times for each of the 30 models\n",
      "   Total queries: 300\n",
      "   Estimated time: ~30 minutes\n",
      "\n",
      " Progress is saved incrementally to logs/control/\n",
      "\n",
      " Starting control experiment...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_control_experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Progress is saved incrementally to logs/control/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Starting control experiment...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m control_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_control_experiment\u001b[49m(\n\u001b[1;32m     11\u001b[0m     models\u001b[38;5;241m=\u001b[39mMODELS,\n\u001b[1;32m     12\u001b[0m     repetitions\u001b[38;5;241m=\u001b[39mCONTROL_REPETITIONS,\n\u001b[1;32m     13\u001b[0m     all_open_router\u001b[38;5;241m=\u001b[39mALL_OPEN_ROUTER,\n\u001b[1;32m     14\u001b[0m     initial_batch_size\u001b[38;5;241m=\u001b[39mINITIAL_BATCH_SIZE,\n\u001b[1;32m     15\u001b[0m     initial_concurrency\u001b[38;5;241m=\u001b[39mINITIAL_CONCURRENCY,\n\u001b[1;32m     16\u001b[0m     max_concurrency\u001b[38;5;241m=\u001b[39mMAX_CONCURRENCY,\n\u001b[1;32m     17\u001b[0m     adaptive_mode\u001b[38;5;241m=\u001b[39mADAPTIVE_MODE,\n\u001b[1;32m     18\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     19\u001b[0m     tag\u001b[38;5;241m=\u001b[39mCONTROL_TAG\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Control experiment complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Results saved to: logs/control/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_control_experiment' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTROL EXPERIMENT: Measuring Within-Model Variance\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n  This will run the same query 10 times for each of the 30 models\")\n",
    "print(\"   Total queries: 300\")\n",
    "print(\"   Estimated time: ~30 minutes\")\n",
    "print(\"\\n Progress is saved incrementally to logs/control/\")\n",
    "print(\"\\n Starting control experiment...\\n\")\n",
    "\n",
    "control_results = run_control_experiment(\n",
    "    models=MODELS,\n",
    "    repetitions=CONTROL_REPETITIONS,\n",
    "    all_open_router=ALL_OPEN_ROUTER,\n",
    "    initial_batch_size=INITIAL_BATCH_SIZE,\n",
    "    initial_concurrency=INITIAL_CONCURRENCY,\n",
    "    max_concurrency=MAX_CONCURRENCY,\n",
    "    adaptive_mode=ADAPTIVE_MODE,\n",
    "    max_retries=5,\n",
    "    tag=CONTROL_TAG\n",
    ")\n",
    "\n",
    "print(\"\\n Control experiment complete!\")\n",
    "print(f\"   Results saved to: logs/control/\")\n",
    "for model in control_results:\n",
    "    total_responses = sum(len(personas) for personas in control_results[model].values())\n",
    "    print(f\"   {model}: {total_responses} responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Control Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Computing control similarities...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'supercompute_similarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Computing control similarities...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m control_matrices, control_dfs, control_personas, control_embeddings \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m----> 4\u001b[0m     \u001b[43msupercompute_similarities\u001b[49m(control_results)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Save similarities\u001b[39;00m\n\u001b[1;32m      7\u001b[0m control_sim_path \u001b[38;5;241m=\u001b[39m save_similarity_results(\n\u001b[1;32m      8\u001b[0m     control_matrices, control_dfs, control_personas, control_embeddings,\n\u001b[1;32m      9\u001b[0m     tag\u001b[38;5;241m=\u001b[39mCONTROL_TAG, subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontrol\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'supercompute_similarities' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Computing control similarities...\")\n",
    "\n",
    "control_matrices, control_dfs, control_personas, control_embeddings = \\\n",
    "    supercompute_similarities(control_results)\n",
    "\n",
    "# Save similarities\n",
    "control_sim_path = save_similarity_results(\n",
    "    control_matrices, control_dfs, control_personas, control_embeddings,\n",
    "    tag=CONTROL_TAG, subdir=\"control\"\n",
    ")\n",
    "\n",
    "print(f\"\\n Control similarities computed and saved!\")\n",
    "print(f\"   Matrices computed: {sum(len(topics) for topics in control_matrices.values())}\")\n",
    "print(f\"   Saved to: {control_sim_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ALT*: Load Cached Control Results\n",
    "\n",
    "**Skip control experiment** and load cached data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE: Load cached control results\n",
    "# Uncomment to use cached data:\n",
    "\n",
    "# control_results = load_latest_fast_results(subdir=\"control\")\n",
    "# if control_results:\n",
    "#     print(\" Loaded control results from cache!\")\n",
    "#     for model in control_results:\n",
    "#         total_responses = sum(len(personas) for personas in control_results[model].values())\n",
    "#         print(f\"   {model}: {total_responses} responses\")\n",
    "    \n",
    "#     # Load similarities\n",
    "#     control_sim_data = load_latest_similarity_results(subdir=\"control\")\n",
    "#     if control_sim_data:\n",
    "#         control_matrices, control_dfs, control_personas, control_embeddings = control_sim_data\n",
    "#         print(f\"\\n Loaded control similarities from cache!\")\n",
    "#     else:\n",
    "#         print(\" No cached control similarities found. Run the cell above to compute.\")\n",
    "# else:\n",
    "#     print(\" No cached control results found. Run the control experiment above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: MAIN EXPERIMENT\n",
    "\n",
    "## Measure Across-Persona Variance\n",
    "\n",
    "Query 100 diverse personas across 15 topics with 30 models.\n",
    "\n",
    "**Estimated time:** 6-12 hours for 45,000 queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MAIN EXPERIMENT: Measuring Across-Persona Variance\n",
      "================================================================================\n",
      "\n",
      " Fetching 2 personas from NVIDIA Nemotron dataset...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_and_clean_personas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_PERSONAS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m personas from NVIDIA Nemotron dataset...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m main_personas \u001b[38;5;241m=\u001b[39m \u001b[43mget_and_clean_personas\u001b[49m(\n\u001b[1;32m      7\u001b[0m     offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      8\u001b[0m     length\u001b[38;5;241m=\u001b[39mNUM_PERSONAS,\n\u001b[1;32m      9\u001b[0m     cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     tag\u001b[38;5;241m=\u001b[39mMAIN_TAG,\n\u001b[1;32m     11\u001b[0m     subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m num_personas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(main_personas\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m'\u001b[39m, []))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_personas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m personas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_and_clean_personas' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MAIN EXPERIMENT: Measuring Across-Persona Variance\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n Fetching {NUM_PERSONAS} personas from NVIDIA Nemotron dataset...\\n\")\n",
    "\n",
    "main_personas = get_and_clean_personas(\n",
    "    offset=0,\n",
    "    length=NUM_PERSONAS,\n",
    "    cache=True,\n",
    "    tag=MAIN_TAG,\n",
    "    subdir=\"main\"\n",
    ")\n",
    "\n",
    "num_personas = len(main_personas.get('rows', []))\n",
    "print(f\"\\n Loaded {num_personas} personas\")\n",
    "print(f\"   Saved to: logs/main/\")\n",
    "\n",
    "# Show sample\n",
    "if main_personas['rows']:\n",
    "    sample = main_personas['rows'][0]['row']\n",
    "    print(f\"\\nSample Persona:\")\n",
    "    print(f\"   Age: {sample.get('age')}, Sex: {sample.get('sex')}\")\n",
    "    print(f\"   Location: {sample.get('city')}, {sample.get('state')}\")\n",
    "    print(f\"   Occupation: {sample.get('occupation')}\")\n",
    "    print(f\"   Persona: {sample.get('persona', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ALT*: Load Cached Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_latest_personas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ALTERNATIVE: Load cached personas\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Uncomment to use cached data:\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m main_personas \u001b[38;5;241m=\u001b[39m \u001b[43mload_latest_personas\u001b[49m(subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m main_personas:\n\u001b[1;32m      6\u001b[0m     num_personas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(main_personas\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m'\u001b[39m, []))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_latest_personas' is not defined"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE: Load cached personas\n",
    "# Uncomment to use cached data:\n",
    "\n",
    "main_personas = load_latest_personas(subdir=\"main\")\n",
    "if main_personas:\n",
    "    num_personas = len(main_personas.get('rows', []))\n",
    "    print(f\" Loaded {num_personas} personas from cache\")\n",
    "else:\n",
    "    print(\" No cached personas found. Run the cell above to fetch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generating queries for 2 topics...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_queries_for_personas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Generating queries for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(TOPICS)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m topics...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m main_queries \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_queries_for_personas\u001b[49m(main_personas, TOPICS)\n\u001b[1;32m      5\u001b[0m total_queries_per_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(topic_queries) \u001b[38;5;28;01mfor\u001b[39;00m topic_queries \u001b[38;5;129;01min\u001b[39;00m main_queries\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_queries_per_model\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m queries per model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_queries_for_personas' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Generating queries for {len(TOPICS)} topics...\\n\")\n",
    "\n",
    "main_queries = generate_queries_for_personas(main_personas, TOPICS)\n",
    "\n",
    "total_queries_per_model = sum(len(topic_queries) for topic_queries in main_queries.values())\n",
    "print(f\" Generated {total_queries_per_model:,} queries per model\")\n",
    "print(f\"   Total across all {len(MODELS)} models: {total_queries_per_model * len(MODELS):,}\")\n",
    "print(f\"\\n   Breakdown:\")\n",
    "for topic in list(main_queries.keys())[:3]:  # Show first 3 topics\n",
    "    print(f\"   - {topic}: {len(main_queries[topic])} queries\")\n",
    "if len(main_queries) > 3:\n",
    "    print(f\"   ... and {len(main_queries) - 3} more topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Query LLMs\n",
    "\n",
    "** WARNING: This will take 6-12 hours and make ~45,000 API calls!**\n",
    "\n",
    "Progress is saved incrementally to `logs/main/incremental/` - you can stop and resume anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERYING LLMs - MAIN EXPERIMENT\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'total_queries_per_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUERYING LLMs - MAIN EXPERIMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  This will make \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_queries_per_model\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(MODELS)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m API calls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Estimated time: ~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_queries_per_model\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(MODELS)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_queries_per_model\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(MODELS)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m3600\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m hours)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Progress saved incrementally to: logs/main/incremental/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_queries_per_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERYING LLMs - MAIN EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n  This will make {total_queries_per_model * len(MODELS):,} API calls\")\n",
    "print(f\"   Estimated time: ~{total_queries_per_model * len(MODELS) / 10 / 60:.1f} minutes ({total_queries_per_model * len(MODELS) / 10 / 3600:.1f} hours)\")\n",
    "print(f\"\\n Progress saved incrementally to: logs/main/incremental/\")\n",
    "print(\"   You can stop and resume anytime using query_llm_fast_resume()\")\n",
    "print(\"\\n Starting main experiment queries...\\n\")\n",
    "\n",
    "main_results = query_llm_fast(\n",
    "    nested_queries=main_queries,\n",
    "    list_of_models=MODELS,\n",
    "    initial_batch_size=INITIAL_BATCH_SIZE,\n",
    "    initial_concurrency=INITIAL_CONCURRENCY,\n",
    "    max_concurrency=MAX_CONCURRENCY,\n",
    "    adaptive_mode=ADAPTIVE_MODE,\n",
    "    all_open_router=ALL_OPEN_ROUTER,\n",
    "    max_retries=5,\n",
    "    ensure_100_percent_success=True,\n",
    "    save_incremental=True,\n",
    "    subdir=\"main\"\n",
    ")\n",
    "\n",
    "print(\"\\n Main experiment complete!\")\n",
    "print(f\"   Results saved to: logs/main/\")\n",
    "for model in list(main_results.keys())[:5]:  # Show first 5 models\n",
    "    total_responses = sum(len(personas) for personas in main_results[model].values())\n",
    "    print(f\"   {model}: {total_responses} responses\")\n",
    "if len(main_results) > 5:\n",
    "    print(f\"   ... and {len(main_results) - 5} more models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESUME: Continue Interrupted Main Experiment\n",
    "\n",
    "**Use this cell if the main experiment was interrupted and you need to resume.**\n",
    "\n",
    "This will:\n",
    "- Load progress from the most recent incremental save\n",
    "- Continue querying from where it left off\n",
    "- Ensure 100% completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RESUME: Run this cell to resume an interrupted experiment\n",
    "# # (Instead of running the main query cell above)\n",
    "\n",
    "# # Import the PATCHED resume function that fixes persona ID type mismatch\n",
    "# import sys\n",
    "# sys.path.insert(0, '.')\n",
    "# from patched_resume import query_llm_fast_resume_fixed\n",
    "\n",
    "# from duplicity import load_incremental_results\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "\n",
    "# # Find the most recent incremental save file\n",
    "# incremental_dir = \"logs/main/incremental\"\n",
    "# incremental_files = sorted(\n",
    "#     Path(incremental_dir).glob(\"incremental_*.json\"),\n",
    "#     key=lambda p: p.stat().st_mtime,\n",
    "#     reverse=True\n",
    "# )\n",
    "\n",
    "# if not incremental_files:\n",
    "#     print(\"  No incremental save files found in logs/main/incremental/\")\n",
    "#     print(\"   Run the main query cell above to start the experiment.\")\n",
    "# else:\n",
    "#     # Find the cleaned file specifically\n",
    "#     cleaned_file = None\n",
    "#     for f in incremental_files:\n",
    "#         if \"cleaned\" in f.name:\n",
    "#             cleaned_file = str(f)\n",
    "#             break\n",
    "    \n",
    "#     if cleaned_file:\n",
    "#         latest_incremental = cleaned_file\n",
    "#         print(f\" Using cleaned file: {latest_incremental}\")\n",
    "#     else:\n",
    "#         latest_incremental = str(incremental_files[0])\n",
    "#         print(f\" Using most recent file: {latest_incremental}\")\n",
    "    \n",
    "#     # Load partial results to see progress - extract 'results' key properly\n",
    "#     data = load_incremental_results(latest_incremental)\n",
    "#     partial_results = data.get('results', {})\n",
    "#     total_expected = len(MODELS) * total_queries_per_model\n",
    "    \n",
    "#     # Fix: Convert persona IDs to strings for counting\n",
    "#     completed = sum(\n",
    "#         len(personas) \n",
    "#         for model_data in partial_results.values() \n",
    "#         for personas in model_data.values()\n",
    "#     )\n",
    "#     print(f\" Progress: {completed}/{total_expected} queries complete ({100*completed/total_expected:.1f}%)\")\n",
    "#     print(f\"\\n Resuming from: {latest_incremental}\")\n",
    "#     print(f\"  Using PATCHED resume function to fix persona ID type mismatch bug\\n\")\n",
    "    \n",
    "#     # Resume the experiment using the FIXED function\n",
    "#     main_results = query_llm_fast_resume_fixed(\n",
    "#         incremental_file_path=latest_incremental,\n",
    "#         original_queries=main_queries,\n",
    "#         list_of_models=MODELS,\n",
    "#         initial_batch_size=INITIAL_BATCH_SIZE,\n",
    "#         initial_concurrency=INITIAL_CONCURRENCY,\n",
    "#         max_concurrency=MAX_CONCURRENCY,\n",
    "#         adaptive_mode=ADAPTIVE_MODE,\n",
    "#         all_open_router=ALL_OPEN_ROUTER,\n",
    "#         max_retries=5,\n",
    "#         ensure_100_percent_success=True,\n",
    "#         save_incremental=True,\n",
    "#         subdir=\"main\"\n",
    "#     )\n",
    "    \n",
    "#     print(\"\\n Resumed experiment complete!\")\n",
    "#     print(f\"   Results saved to: logs/main/\")\n",
    "#     for model in list(main_results.keys())[:5]:\n",
    "#         total_responses = sum(len(personas) for personas in main_results[model].values())\n",
    "#         print(f\"   {model}: {total_responses} responses\")\n",
    "#     if len(main_results) > 5:\n",
    "#         print(f\"   ... and {len(main_results) - 5} more models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ALT*: Load Cached Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load specific cached main results file\n",
    "# # Load from a specific file instead of auto-discovering\n",
    "\n",
    "# import json\n",
    "\n",
    "# # Specify the exact file you want to load\n",
    "# specific_file = \"logs/main/final/responses_full2_final.json\"\n",
    "\n",
    "# print(f\" Loading specific file: {specific_file}\")\n",
    "\n",
    "# try:\n",
    "#     with open(specific_file, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "        \n",
    "#         # Check if it's a raw results dict or has a 'results' key\n",
    "#         if 'results' in data:\n",
    "#             main_results = data['results']\n",
    "#         else:\n",
    "#             main_results = data\n",
    "    \n",
    "#     print(\" Loaded main results from specific file!\")\n",
    "#     print(f\"   Models: {len(main_results)}\")\n",
    "    \n",
    "#     # Show first 5 models\n",
    "#     for model in list(main_results.keys())[:5]:\n",
    "#         total_responses = sum(len(personas) for personas in main_results[model].values())\n",
    "#         print(f\"   {model}: {total_responses} responses\")\n",
    "    \n",
    "#     if len(main_results) > 5:\n",
    "#         print(f\"   ... and {len(main_results) - 5} more models\")\n",
    "        \n",
    "#     # Calculate total responses\n",
    "#     total_responses = sum(\n",
    "#         len(personas)\n",
    "#         for model_data in main_results.values()\n",
    "#         for personas in model_data.values()\n",
    "#     )\n",
    "#     print(f\"\\n   Total responses: {total_responses:,}\")\n",
    "    \n",
    "# except FileNotFoundError:\n",
    "#     print(f\"  File not found: {specific_file}\")\n",
    "#     print(\"   Please check the file path and try again.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"  Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute Main Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Computing main experiment similarities...\n",
      "   This will compute embeddings and similarity matrices for all model/topic combinations\n",
      "   Estimated time: 5-10 minutes\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'supercompute_similarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   This will compute embeddings and similarity matrices for all model/topic combinations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Estimated time: 5-10 minutes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m main_matrices, main_dfs, main_personas_ids, main_embeddings \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m----> 6\u001b[0m     \u001b[43msupercompute_similarities\u001b[49m(main_results)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Save similarities\u001b[39;00m\n\u001b[1;32m      9\u001b[0m main_sim_path \u001b[38;5;241m=\u001b[39m save_similarity_results(\n\u001b[1;32m     10\u001b[0m     main_matrices, main_dfs, main_personas_ids, main_embeddings,\n\u001b[1;32m     11\u001b[0m     tag\u001b[38;5;241m=\u001b[39mMAIN_TAG, subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'supercompute_similarities' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Computing main experiment similarities...\")\n",
    "print(\"   This will compute embeddings and similarity matrices for all model/topic combinations\")\n",
    "print(\"   Estimated time: 5-10 minutes\\n\")\n",
    "\n",
    "main_matrices, main_dfs, main_personas_ids, main_embeddings = \\\n",
    "    supercompute_similarities(main_results)\n",
    "\n",
    "# Save similarities\n",
    "main_sim_path = save_similarity_results(\n",
    "    main_matrices, main_dfs, main_personas_ids, main_embeddings,\n",
    "    tag=MAIN_TAG, subdir=\"main\"\n",
    ")\n",
    "\n",
    "print(f\"\\n Main similarities computed and saved!\")\n",
    "print(f\"   Matrices computed: {sum(len(topics) for topics in main_matrices.values())}\")\n",
    "print(f\"   Saved to: {main_sim_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ALT*: Load Cached Main Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load cached main similarities\n",
    "# # Uncomment to use cached data:\n",
    "\n",
    "# main_sim_data = load_latest_similarity_results(subdir=\"main\")\n",
    "# if main_sim_data:\n",
    "#     main_matrices, main_dfs, main_personas_ids, main_embeddings = main_sim_data\n",
    "#     print(\" Loaded main similarities from cache!\")\n",
    "#     print(f\"   Matrices computed: {sum(len(topics) for topics in main_matrices.values())}\")\n",
    "# else:\n",
    "#     print(\" No cached main similarities found. Run the cell above to compute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: VARIANCE ANALYSIS\n",
    "\n",
    "## Compare Control vs. Persona Variance\n",
    "\n",
    "Analyze how models vary internally (control) vs. across personas (main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VARIANCE ANALYSIS: Control vs. Persona Variance\n",
      "================================================================================\n",
      "\n",
      " Computing within-model variance (control)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_within_model_variance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Computing within-model variance (control)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m control_variance \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_within_model_variance\u001b[49m(\n\u001b[1;32m     10\u001b[0m     control_results,\n\u001b[1;32m     11\u001b[0m     control_matrices\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m control_variance\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/variance_comparison/control_variance.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Mean control similarity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontrol_variance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_within_model_variance' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"output/variance_comparison\", exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE ANALYSIS: Control vs. Persona Variance\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n Computing within-model variance (control)...\")\n",
    "\n",
    "control_variance = compute_within_model_variance(\n",
    "    control_results,\n",
    "    control_matrices\n",
    ")\n",
    "\n",
    "control_variance.to_csv(\"output/variance_comparison/control_variance.csv\", index=False)\n",
    "print(f\"   Mean control similarity: {control_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Std dev: {control_variance['mean_similarity'].std():.4f}\")\n",
    "print(f\"   Saved to: output/variance_comparison/control_variance.csv\")\n",
    "\n",
    "print(\"\\n Computing across-persona variance (main)...\")\n",
    "\n",
    "persona_variance = compute_across_persona_variance(\n",
    "    main_results,\n",
    "    main_matrices\n",
    ")\n",
    "\n",
    "persona_variance.to_csv(\"output/variance_comparison/persona_variance.csv\", index=False)\n",
    "print(f\"   Mean persona similarity: {persona_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Std dev: {persona_variance['mean_similarity'].std():.4f}\")\n",
    "print(f\"   Saved to: output/variance_comparison/persona_variance.csv\")\n",
    "\n",
    "print(\"\\n Variance computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating variance comparison visualizations...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_variance_comparison_visualizations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Creating variance comparison visualizations...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcreate_variance_comparison_visualizations\u001b[49m(\n\u001b[1;32m      4\u001b[0m     control_variance,\n\u001b[1;32m      5\u001b[0m     persona_variance,\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/variance_comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Visualizations created!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Output files:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_variance_comparison_visualizations' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Creating variance comparison visualizations...\\n\")\n",
    "\n",
    "create_variance_comparison_visualizations(\n",
    "    control_variance,\n",
    "    persona_variance,\n",
    "    output_dir=\"output/variance_comparison\"\n",
    ")\n",
    "\n",
    "print(\"\\n Visualizations created!\")\n",
    "print(\"   Output files:\")\n",
    "print(\"   - comparison_bar_chart.png\")\n",
    "print(\"   - comparison_scatter.png\")\n",
    "print(\"   - comparison_heatmap.png\")\n",
    "print(\"   - variance_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Variance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generating variance analysis report...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_variance_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Generating variance analysis report...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_variance_report\u001b[49m(\n\u001b[1;32m      4\u001b[0m     control_variance,\n\u001b[1;32m      5\u001b[0m     persona_variance,\n\u001b[1;32m      6\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/variance_comparison/report.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVARIANCE ANALYSIS COMPLETE!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_variance_report' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Generating variance analysis report...\\n\")\n",
    "\n",
    "report = generate_variance_report(\n",
    "    control_variance,\n",
    "    persona_variance,\n",
    "    output_path=\"output/variance_comparison/report.txt\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n All variance analysis files saved to: output/variance_comparison/\")\n",
    "print(\"\\n Key Insights:\")\n",
    "print(f\"   Control mean similarity: {control_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Persona mean similarity: {persona_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Difference: {abs(control_variance['mean_similarity'].mean() - persona_variance['mean_similarity'].mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creative Visualizations: Control vs Main Experiment\n",
    "\n",
    "**Additional custom plots to illustrate the differences between control and persona experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating variance comparison visualizations...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'control_matrices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variance comparison visualizations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This function creates all 5 variance plots\u001b[39;00m\n\u001b[1;32m      6\u001b[0m create_all_variance_plots(\n\u001b[0;32m----> 7\u001b[0m     control_similarities\u001b[38;5;241m=\u001b[39m\u001b[43mcontrol_matrices\u001b[49m,\n\u001b[1;32m      8\u001b[0m     persona_similarities\u001b[38;5;241m=\u001b[39mmain_matrices,\n\u001b[1;32m      9\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariance_plots\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m All variance plots created successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   See variance_plots/ directory for:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'control_matrices' is not defined"
     ]
    }
   ],
   "source": [
    "from duplicity.variance_visualizations import create_all_variance_plots\n",
    "\n",
    "print(\"Creating variance comparison visualizations...\")\n",
    "\n",
    "# This function creates all 5 variance plots\n",
    "create_all_variance_plots(\n",
    "    control_similarities=control_matrices,\n",
    "    persona_similarities=main_matrices,\n",
    "    output_dir=\"variance_plots\"\n",
    ")\n",
    "\n",
    "print(\" All variance plots created successfully!\")\n",
    "print(\"   See variance_plots/ directory for:\")\n",
    "print(\"   - 1_quadrant_plot.png\")\n",
    "print(\"   - 2_sensitivity_ranking.png\")\n",
    "print(\"   - 3_landscape_zones.png\")\n",
    "print(\"   - 4_distribution_violin.png\")\n",
    "print(\"   - 5_consistency_variability.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 4: STANDARD ANALYSES\n",
    "\n",
    "## Traditional ConsistencyAI visualizations and analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Similarity Scores (Main Experiment):\n",
      "   (Higher = more consistent across personas)\n",
      "\n",
      "\n",
      "x-ai/grok-4:\n",
      "   Are COVID-19 vaccines safe and effective?: 0.8841\n",
      "   Is the U.S.-Mexico border secure?: 0.9301\n",
      "   Overall: 0.9071\n",
      "\n",
      "x-ai/grok-3:\n",
      "   Are COVID-19 vaccines safe and effective?: 0.9089\n",
      "   Is the U.S.-Mexico border secure?: 0.9329\n",
      "   Overall: 0.9209\n"
     ]
    }
   ],
   "source": [
    "# Compute average scores from main experiment\n",
    "avg_scores = collect_avg_scores_by_model(main_matrices)\n",
    "\n",
    "print(\"\\nAverage Similarity Scores (Main Experiment):\")\n",
    "print(\"   (Higher = more consistent across personas)\\n\")\n",
    "\n",
    "for model in list(avg_scores.keys())[:5]:  # Show first 5\n",
    "    print(f\"\\n{model}:\")\n",
    "    for topic in list(avg_scores[model].keys())[:3]:  # Show first 3 topics\n",
    "        score = avg_scores[model][topic]\n",
    "        print(f\"   {topic}: {score:.4f}\")\n",
    "    if len(avg_scores[model]) > 3:\n",
    "        print(f\"   ... and {len(avg_scores[model]) - 3} more topics\")\n",
    "    overall = np.mean(list(avg_scores[model].values()))\n",
    "    print(f\"   Overall: {overall:.4f}\")\n",
    "\n",
    "if len(avg_scores) > 5:\n",
    "    print(f\"\\n... and {len(avg_scores) - 5} more models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Overall Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating overall leaderboard...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_overall_leaderboard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Creating overall leaderboard...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplot_overall_leaderboard\u001b[49m(avg_scores, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Leaderboard saved to: output/overall_leaderboard.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_overall_leaderboard' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Creating overall leaderboard...\\n\")\n",
    "\n",
    "plot_overall_leaderboard(avg_scores, save_path=\"output\", show=True)\n",
    "\n",
    "print(\"\\n Leaderboard saved to: output/overall_leaderboard.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Topic-Specific Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating topic-specific plots...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_similarity_by_sphere' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Creating topic-specific plots...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplot_similarity_by_sphere\u001b[49m(avg_scores, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Topic plots saved to: output/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_similarity_by_sphere' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Creating topic-specific plots...\\n\")\n",
    "\n",
    "plot_similarity_by_sphere(avg_scores, save_path=\"output\")\n",
    "\n",
    "print(\"\\n Topic plots saved to: output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Similarity Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating similarity heatmaps...\n",
      "   (This creates one heatmap per model/topic combination)\n",
      "   Processing in SMALL batches with ULTRA-AGGRESSIVE memory cleanup\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'main_matrices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Collect all model/topic combinations\u001b[39;00m\n\u001b[1;32m     14\u001b[0m all_combinations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmain_matrices\u001b[49m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m main_matrices[model]:\n\u001b[1;32m     17\u001b[0m         all_combinations\u001b[38;5;241m.\u001b[39mappend((model, topic))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'main_matrices' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "\n",
    "print(\"\\n Creating similarity heatmaps...\")\n",
    "print(\"   (This creates one heatmap per model/topic combination)\")\n",
    "print(\"   Processing in SMALL batches with ULTRA-AGGRESSIVE memory cleanup\\n\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 3  # Only 3 at a time - very conservative\n",
    "heatmap_count = 0\n",
    "\n",
    "# Collect all model/topic combinations\n",
    "all_combinations = []\n",
    "for model in main_matrices:\n",
    "    for topic in main_matrices[model]:\n",
    "        all_combinations.append((model, topic))\n",
    "\n",
    "total_heatmaps = len(all_combinations)\n",
    "print(f\"Total heatmaps to create: {total_heatmaps}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (ultra-conservative)\")\n",
    "print(f\"This will take longer but should avoid crashes\\n\")\n",
    "\n",
    "# Process in tiny batches\n",
    "for batch_start in range(0, total_heatmaps, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, total_heatmaps)\n",
    "    batch_num = (batch_start // BATCH_SIZE) + 1\n",
    "    total_batches = (total_heatmaps + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    print(f\"Batch {batch_num}/{total_batches} (heatmaps {batch_start+1}-{batch_end})...\", end=' ')\n",
    "    \n",
    "    # Create heatmaps for this batch\n",
    "    for model, topic in all_combinations[batch_start:batch_end]:\n",
    "        safe_model = model.replace(\"/\", \"_\")\n",
    "        safe_topic = topic.replace(\" \", \"_\").replace(\"?\", \"\").replace(\"'\", \"\")\n",
    "        save_path = f\"output/heatmap_{safe_model}_{safe_topic}.png\"\n",
    "        \n",
    "        fig, ax = plot_similarity_matrix_with_values(\n",
    "            similarity_matrix=main_matrices[model][topic],\n",
    "            persona_ids=main_personas_ids[model][topic],\n",
    "            show_values=(len(main_personas_ids[model][topic]) <= 20),\n",
    "            model_name=model,\n",
    "            topic=topic,\n",
    "            save_path=save_path\n",
    "        )\n",
    "        plt.close(fig)\n",
    "        del fig, ax  # Explicit deletion\n",
    "        heatmap_count += 1\n",
    "    \n",
    "    # ULTRA-AGGRESSIVE memory cleanup\n",
    "    plt.close('all')  # Close ALL figures\n",
    "    plt.clf()  # Clear current figure\n",
    "    plt.cla()  # Clear current axes\n",
    "    gc.collect()  # Force garbage collection\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "print(f\"\\n Created {heatmap_count} heatmaps in: output/\")\n",
    "print(f\"   All heatmaps saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performing clustering analysis...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'analyze_and_cluster_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Performing clustering analysis...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m analysis_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_and_cluster_embeddings\u001b[49m(\n\u001b[1;32m      4\u001b[0m     all_embeddings\u001b[38;5;241m=\u001b[39mmain_embeddings,\n\u001b[1;32m      5\u001b[0m     all_similarity_matrices\u001b[38;5;241m=\u001b[39mmain_matrices,\n\u001b[1;32m      6\u001b[0m     all_sorted_personas\u001b[38;5;241m=\u001b[39mmain_personas_ids,\n\u001b[1;32m      7\u001b[0m     max_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m      9\u001b[0m     save_plots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     plots_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/clustering\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Clustering analysis complete!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyze_and_cluster_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Performing clustering analysis...\\n\")\n",
    "\n",
    "analysis_results = analyze_and_cluster_embeddings(\n",
    "    all_embeddings=main_embeddings,\n",
    "    all_similarity_matrices=main_matrices,\n",
    "    all_sorted_personas=main_personas_ids,\n",
    "    max_clusters=10,\n",
    "    random_state=42,\n",
    "    save_plots=True,\n",
    "    plots_dir=\"output/clustering\"\n",
    ")\n",
    "\n",
    "print(\"\\n Clustering analysis complete!\\n\")\n",
    "\n",
    "# Print summary\n",
    "print_analysis_summary(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running central analysis...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_central_analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Running central analysis...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m per_model_per_topic, model_overall_weighted, topic_across_models_weighted, benchmark \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mcompute_central_analysis\u001b[49m(main_matrices, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Central analysis complete!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_central_analysis' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Running central analysis...\\n\")\n",
    "\n",
    "per_model_per_topic, model_overall_weighted, topic_across_models_weighted, benchmark = \\\n",
    "    compute_central_analysis(main_matrices, output_dir=\"output/analysis\")\n",
    "\n",
    "print(\"\\n Central analysis complete!\\n\")\n",
    "\n",
    "# Display results\n",
    "print_central_analysis_summary(model_overall_weighted, topic_across_models_weighted, benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EXPERIMENT COMPLETE!\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "### What you just did:\n",
    "\n",
    "**Control Experiment:**\n",
    "- Measured within-model variance using Mary Alberti persona\n",
    "- Ran same query 10 times per model (300 total queries)\n",
    "- Identified most internally consistent models\n",
    "\n",
    "**Main Experiment:**\n",
    "- Loaded 100 diverse personas from NVIDIA Nemotron\n",
    "- Generated personalized queries across 15 sensitive topics\n",
    "- Queried 30 LLMs (45,000 total queries)\n",
    "- Measured across-persona variance\n",
    "\n",
    "**Variance Analysis:**\n",
    "- Compared control vs. persona variance\n",
    "- Identified most persona-sensitive models\n",
    "- Generated comprehensive visualizations and reports\n",
    "\n",
    "**Standard Analysis:**\n",
    "- Created similarity heatmaps\n",
    "- Generated leaderboards and topic plots\n",
    "- Performed clustering analysis\n",
    "- Computed central analysis metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Output Files\n",
    "\n",
    "### Control Experiment (`logs/control/`):\n",
    "- Query results and similarities\n",
    "- Incremental progress files\n",
    "\n",
    "### Main Experiment (`logs/main/`):\n",
    "- Persona data (100 personas)\n",
    "- Query results (45,000 responses)\n",
    "- Similarity matrices and embeddings\n",
    "- Incremental progress files\n",
    "\n",
    "### Variance Analysis (`output/variance_comparison/`):\n",
    "- `control_variance.csv` - Within-model variance stats\n",
    "- `persona_variance.csv` - Across-persona variance stats\n",
    "- `comparison_bar_chart.png` - Side-by-side comparison\n",
    "- `comparison_scatter.png` - Control vs. persona scatter\n",
    "- `comparison_heatmap.png` - Metrics heatmap\n",
    "- `variance_distributions.png` - Distribution box plots\n",
    "- `report.txt` - Detailed text report\n",
    "\n",
    "### Standard Analysis (`output/`):\n",
    "- Similarity heatmaps (one per model/topic)\n",
    "- Overall leaderboard\n",
    "- Topic-specific plots\n",
    "- Clustering visualizations (`output/clustering/`)\n",
    "- Central analysis CSVs (`output/analysis/`)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "**Understanding Variance:**\n",
    "- **High control variance** = Model is noisy/inconsistent with itself\n",
    "- **Low control variance** = Model is stable and consistent\n",
    "- **High persona variance** = Model adapts responses to different personas\n",
    "- **Low persona variance** = Model gives similar responses regardless of persona\n",
    "\n",
    "**Ideal Model Profile:**\n",
    "-  High control similarity (consistent with itself)\n",
    "-  Lower persona similarity (persona-aware)\n",
    "- = Stable behavior that adapts to demographic differences\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review variance comparison visualizations in `output/variance_comparison/`\n",
    "2. Read the detailed report: `output/variance_comparison/report.txt`\n",
    "3. Examine model-specific heatmaps in `output/`\n",
    "4. Analyze central analysis results in `output/analysis/`\n",
    "5. Share your findings!\n",
    "\n",
    "---\n",
    "\n",
    "**Built by the Duke Phishermen**  \n",
    "Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute\n",
    "\n",
    "With inquiries, questions, & feedback, please contact: peter dot banyas at duke dot edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
