{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConsistencyAI - Complete Pipeline with Control Experiment\n",
    "\n",
    "**A benchmark for evaluating LLM consistency across demographics + within-model variance**\n",
    "\n",
    "By: Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute (The Duke Phishermen)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook demonstrates the complete ConsistencyAI workflow with both experiments:\n",
    "\n",
    "**Control Experiment (30 min):**\n",
    "1. Load Mary Alberti persona\n",
    "2. Query each model 10 times with the same prompt\n",
    "3. Measure within-model variance (consistency)\n",
    "\n",
    "**Main Experiment (6-12 hours):**\n",
    "1. Load 100 diverse personas from NVIDIA Nemotron dataset\n",
    "2. Generate personalized queries for each persona\n",
    "3. Query 30 LLMs across 15 topics\n",
    "4. Measure across-persona variance (persona sensitivity)\n",
    "\n",
    "**Variance Analysis:**\n",
    "1. Compare control vs. persona variance\n",
    "2. Identify most consistent models\n",
    "3. Identify most persona-sensitive models\n",
    "4. Generate comprehensive visualizations and reports\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "**For a full experimental run:** Execute each cell in order (this will take 6-12 hours)\n",
    "\n",
    "**To use cached data:** Load from cache cells instead of running experiments\n",
    "\n",
    "**To customize:** Edit the configuration in the setup cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies checked/installed\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (if needed)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'requirements.txt'], check=False)\n",
    "    print(\"Dependencies checked/installed\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not install dependencies: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n",
      "ConsistencyAI ready to use\n",
      "Jupyter compatibility enabled\n"
     ]
    }
   ],
   "source": [
    "# Import the ConsistencyAI package\n",
    "from duplicity import (\n",
    "    # Personas\n",
    "    get_and_clean_personas,\n",
    "    generate_queries_for_personas,\n",
    "    load_latest_personas,\n",
    "    \n",
    "    # Queries\n",
    "    query_llm_fast,\n",
    "    query_llm_fast_resume,\n",
    "    load_latest_results,\n",
    "    load_latest_fast_results,\n",
    "    \n",
    "    # Similarity\n",
    "    supercompute_similarities,\n",
    "    collect_avg_scores_by_model,\n",
    "    save_similarity_results,\n",
    "    load_latest_similarity_results,\n",
    "    load_similarity_results,\n",
    "    \n",
    "    # Visualization\n",
    "    plot_similarity_matrix_with_values,\n",
    "    plot_overall_leaderboard,\n",
    "    plot_similarity_by_sphere,\n",
    "    Embedding3DVisualizer,\n",
    "    \n",
    "    # Advanced Analysis\n",
    "    analyze_and_cluster_embeddings,\n",
    "    print_analysis_summary,\n",
    "    \n",
    "    # Central Analysis\n",
    "    compute_central_analysis,\n",
    "    print_central_analysis_summary,\n",
    "    \n",
    "    # Control Experiment (NEW)\n",
    "    run_control_experiment,\n",
    "    \n",
    "    # Variance Analysis (NEW)\n",
    "    compute_within_model_variance,\n",
    "    compute_across_persona_variance,\n",
    "    create_variance_comparison_visualizations,\n",
    "    generate_variance_report,\n",
    "    \n",
    "    # Configuration\n",
    "    config\n",
    ")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable nested event loops for Jupyter compatibility\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"All imports successful\")\n",
    "print(\"ConsistencyAI ready to use\")\n",
    "print(\"Jupyter compatibility enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key Configuration\n",
    "\n",
    "**Important:** If you've already imported the duplicity package and then set your API key, you need to restart the kernel for the changes to take effect.\n",
    "\n",
    "To restart: Kernel â†’ Restart & Clear Output, then re-run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENROUTER_API_KEY is set\n",
      "âœ… OPENAI_API_KEY is set\n"
     ]
    }
   ],
   "source": [
    "# Set your API keys here or via environment variables\n",
    "# Uncomment and add your keys:\n",
    "# config.set_openrouter_key(\"your-key-here\")\n",
    "# config.set_openai_key(\"your-key-here\")  # Optional\n",
    "\n",
    "# Check if API key is set\n",
    "if config.OPENROUTER_API_KEY:\n",
    "    print(\"âœ… OPENROUTER_API_KEY is set\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: OPENROUTER_API_KEY is not set\")\n",
    "    print(\"Set it with: config.set_openrouter_key('your-key')\")\n",
    "if config.OPENAI_API_KEY:\n",
    "    print(\"âœ… OPENAI_API_KEY is set\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: OPENAI_API_KEY is not set\")\n",
    "    print(\"Set it with: config.set_openai_key('your-key')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment Configuration\n",
    "\n",
    "**Full experimental suite from config.md Example 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini test to make sure everything's working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # LITTLE TEST\n",
    "# # ============================================================\n",
    "\n",
    "# # Number of personas for main experiment\n",
    "# NUM_PERSONAS = 2\n",
    "\n",
    "# # Topics to query about (15 topics)\n",
    "# TOPICS = [\n",
    "#    \"Are COVID-19 vaccines safe and effective?\",\n",
    "#     \"Is the U.S.-Mexico border secure?\",\n",
    "#     ]\n",
    "\n",
    "# # Models to test (30 models)\n",
    "# MODELS = [\n",
    "#     \"x-ai/grok-4\",\n",
    "#     \"x-ai/grok-3\",\n",
    "\n",
    "#     \"anthropic/claude-opus-4.1\",\n",
    "#     \"anthropic/claude-sonnet-4.5\",\n",
    "#     \"anthropic/claude-haiku-4.5\",\n",
    "#     \"anthropic/claude-3.7-sonnet\",\n",
    "#     \"anthropic/claude-3.5-haiku\",\n",
    "\n",
    "#     \"google/gemini-2.5-pro\",\n",
    "#     \"google/gemini-2.5-flash\",\n",
    "#     \"google/gemma-3n-e4b-it\",\n",
    "\n",
    "#     \"openai/gpt-5-pro-2025-10-06\",\n",
    "#     \"openai/gpt-5-chat-latest\",\n",
    "#     \"openai/gpt-5-nano-2025-08-07\",\n",
    "#     \"openai/gpt-4o-2024-08-06\",\n",
    "    \n",
    "#     \"perplexity/sonar-pro-search\",\n",
    "#     \"perplexity/sonar-deep-research\",\n",
    "#     \"perplexity/sonar\",\n",
    "\n",
    "#     \"mistralai/mistral-medium-3.1\",\n",
    "#     \"mistralai/mistral-saba\", # this model is designed for Middle East & South Asia, could be super interesting if we ask about Israel\n",
    "#     \"mistralai/mistral-nemo\",\n",
    "    \n",
    "#     \"qwen/qwen3-max\",\n",
    "#     \"qwen/qwen3-next-80b-a3b-thinking\",\n",
    "#     \"qwen/qwen-turbo\",\n",
    "\n",
    "#     \"deepseek/deepseek-v3.2-exp\",\n",
    "#     \"deepseek/deepseek-v3.1-terminus\",\n",
    "#     \"deepseek/deepseek-r1-0528\",\n",
    "\n",
    "#     \"meta-llama/llama-4-maverick\",\n",
    "#     \"meta-llama/llama-4-scout\",\n",
    "#     \"meta-llama/llama-3.3-70b-instruct\",\n",
    "#     ]\n",
    "\n",
    "# # API routing configuration\n",
    "# ALL_OPEN_ROUTER = True  # Routes non-OpenAI through OpenRouter (OpenAI always goes direct)\n",
    "\n",
    "# # Performance settings\n",
    "# INITIAL_BATCH_SIZE = 30\n",
    "# INITIAL_CONCURRENCY = 10\n",
    "# MAX_CONCURRENCY = 500\n",
    "# ADAPTIVE_MODE = True\n",
    "\n",
    "# # Control experiment settings\n",
    "# CONTROL_REPETITIONS = 2  # Number of times to repeat each model query\n",
    "\n",
    "# # Experiment tags\n",
    "# CONTROL_TAG = \"control\"\n",
    "# MAIN_TAG = \"main\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual experimental config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Number of personas for main experiment\n",
    "NUM_PERSONAS = 100\n",
    "\n",
    "# Topics to query about (15 topics)\n",
    "TOPICS = [\n",
    "    \"Are COVID-19 vaccines safe and effective?\",\n",
    "    \"Is the U.S.-Mexico border secure?\",\n",
    "    \"How has AI impacted employment rates?\",\n",
    "    \"What are the causes of the Israeli-Palestinian conflict?\",\n",
    "    \"What are the changes in abortion access?\",\n",
    "    \"How has racial affirmative action impacted education?\",\n",
    "    \"What are the health impacts of genetically modified food?\",\n",
    "    \"What are the impacts of gun control laws on violent crime?\",\n",
    "    \"How are housing costs changing in America?\",\n",
    "    \"What are the statistics on illegal immigrant crime rates?\",\n",
    "    \"What are the U.S. government's policies on domestic personal information collection?\",\n",
    "    \"Give me statistics on wealth inequality.\",\n",
    "    \"Give me statistics on climate change.\",\n",
    "    \"Give me statistics on the U.S. trade deficit.\",\n",
    "    \"How has breaking up monopolies impacted economic growth?\",]\n",
    "\n",
    "# Models to test (30 models)\n",
    "MODELS = [\n",
    "    \"x-ai/grok-4\",\n",
    "    \"x-ai/grok-3\",\n",
    "\n",
    "    \"anthropic/claude-opus-4.1\",\n",
    "    \"anthropic/claude-sonnet-4.5\",\n",
    "    \"anthropic/claude-haiku-4.5\",\n",
    "    \"anthropic/claude-3.7-sonnet\",\n",
    "    \"anthropic/claude-3.5-haiku\",\n",
    "\n",
    "    \"google/gemini-2.5-pro\",\n",
    "    \"google/gemini-2.5-flash\",\n",
    "    \"google/gemma-3n-e4b-it\",\n",
    "\n",
    "    \"openai/gpt-5-chat-latest\",\n",
    "    \"openai/gpt-4o-2024-08-06\",\n",
    "    \n",
    "    # \"perplexity/sonar-pro-search\",\n",
    "    # \"perplexity/sonar-deep-research\",\n",
    "    \"perplexity/sonar\",\n",
    "\n",
    "    \"mistralai/mistral-medium-3.1\",\n",
    "    \"mistralai/mistral-saba\", # this model is designed for Middle East & South Asia, could be super interesting if we ask about Israel\n",
    "    \"mistralai/mistral-nemo\",\n",
    "    \n",
    "    \"qwen/qwen3-max\",\n",
    "    \"qwen/qwen3-next-80b-a3b-thinking\",\n",
    "    \"qwen/qwen-turbo\",\n",
    "\n",
    "    \"deepseek/deepseek-v3.2-exp\",\n",
    "    \"deepseek/deepseek-v3.1-terminus\",\n",
    "    \"deepseek/deepseek-r1-0528\",\n",
    "\n",
    "    \"meta-llama/llama-4-maverick\",\n",
    "    \"meta-llama/llama-4-scout\",\n",
    "    \"meta-llama/llama-3.3-70b-instruct\",\n",
    "    ]\n",
    "\n",
    "# API routing configuration\n",
    "ALL_OPEN_ROUTER = True  # Routes non-OpenAI through OpenRouter (OpenAI always goes direct)\n",
    "\n",
    "# Performance settings\n",
    "INITIAL_BATCH_SIZE = 30\n",
    "INITIAL_CONCURRENCY = 10\n",
    "MAX_CONCURRENCY = 500\n",
    "ADAPTIVE_MODE = True\n",
    "\n",
    "# Control experiment settings\n",
    "CONTROL_REPETITIONS = 20  # Number of times to repeat each model query\n",
    "\n",
    "# Experiment tags\n",
    "CONTROL_TAG = \"control\"\n",
    "MAIN_TAG = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT CONFIGURATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Control Experiment:\n",
      "   Persona: Mary Alberti\n",
      "   Topic: GMO health impacts\n",
      "   Repetitions per model: 20\n",
      "   Models: 27\n",
      "   Total control queries: 540\n",
      "   Estimated time: ~30 minutes\n",
      "\n",
      "Main Experiment:\n",
      "   Personas: 100\n",
      "   Topics: 15\n",
      "   Models: 27\n",
      "   Total main queries: 40,500\n",
      "   Estimated time: ~67.5 minutes (1.1 hours)\n",
      "\n",
      "================================================================================\n",
      "TOTAL QUERIES: 41,040\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nControl Experiment:\")\n",
    "print(f\"   Persona: Mary Alberti\")\n",
    "print(f\"   Topic: GMO health impacts\")\n",
    "print(f\"   Repetitions per model: {CONTROL_REPETITIONS}\")\n",
    "print(f\"   Models: {len(MODELS)}\")\n",
    "print(f\"   Total control queries: {CONTROL_REPETITIONS * len(MODELS)}\")\n",
    "print(f\"   Estimated time: ~30 minutes\")\n",
    "\n",
    "print(\"\\nMain Experiment:\")\n",
    "print(f\"   Personas: {NUM_PERSONAS}\")\n",
    "print(f\"   Topics: {len(TOPICS)}\")\n",
    "print(f\"   Models: {len(MODELS)}\")\n",
    "total_main_queries = NUM_PERSONAS * len(TOPICS) * len(MODELS)\n",
    "print(f\"   Total main queries: {total_main_queries:,}\")\n",
    "print(f\"   Estimated time: ~{total_main_queries / 10 / 60:.1f} minutes ({total_main_queries / 10 / 3600:.1f} hours)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOTAL QUERIES: {CONTROL_REPETITIONS * len(MODELS) + total_main_queries:,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 1: CONTROL EXPERIMENT\n",
    "\n",
    "## Measure Within-Model Variance\n",
    "\n",
    "Run the same prompt 10 times for each model to measure consistency.\n",
    "\n",
    "**Persona:** Mary Alberti (28-year-old female, Madison, WI, fast food worker)  \n",
    "**Topic:** \"What are the health impacts of genetically modified food?\"\n",
    "\n",
    "**Estimated time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONTROL EXPERIMENT: Measuring Within-Model Variance\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  This will run the same query 10 times for each of the 30 models\n",
      "   Total queries: 300\n",
      "   Estimated time: ~30 minutes\n",
      "\n",
      "ğŸ”„ Progress is saved incrementally to logs/control/\n",
      "\n",
      "ğŸš€ Starting control experiment...\n",
      "\n",
      "============================================================\n",
      "CONTROL EXPERIMENT: Within-Model Variance\n",
      "============================================================\n",
      "   Persona: Mary Alberti (e7c0574639a244c8972c92aab9501035)\n",
      "   Topic: What are the health impacts of genetically modified food?\n",
      "   Repetitions per model: 20\n",
      "   Models: 27\n",
      "   Total queries: 540\n",
      "\n",
      "ğŸ“‹ Loading Mary Alberti persona...\n",
      "ğŸ”„ Generating 20 repetitions...\n",
      "ğŸš€ Querying models...\n",
      "ğŸ¯ Starting FAST robust query processing (100% success mode):\n",
      "   Models: 27\n",
      "   Topics: 1\n",
      "   Personas per topic: 20\n",
      "   Total queries: 540\n",
      "   Initial batch size: 30\n",
      "   Initial concurrency: 10\n",
      "   Max concurrency: 500\n",
      "   Adaptive mode: True\n",
      "   All OpenRouter: True\n",
      "   Max retries: 5\n",
      "   100% success mode: True\n",
      "   Incremental saving: True\n",
      "   Incremental interval: Every 1 batch(es)\n",
      "   Incremental folder: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental\n",
      "ğŸ“‹ Created 540 expected task combinations (modelÃ—topicÃ—persona)\n",
      "   Total batches: 18\n",
      "ğŸš€ Processing batch 1/18 (30 queries, concurrency: 10, current batch size: 30)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 1 complete: 30 success, 0 failed, 20808 tokens, 51.9s\n",
      "ğŸ“ˆ Increasing batch size from 30 to 40 (success rate: 100.0%)\n",
      "ğŸ“ Progress saved: 1/18 batches complete (ETA: 0:14:42)\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch001_batch_001.json\n",
      "   Progress: 1/18 (5.6%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch001_batch_001.json\n",
      "ğŸ“Š Progress: 30/540 (5.6%)\n",
      "   Speed: 0.6 queries/sec | Success: 100.0%\n",
      "   ETA: 14m | Concurrency: 10 | Batch size: 40\n",
      "â³ Minimal delay: 0.1s (excellent performance: 100.0%)\n",
      "ğŸš€ Processing batch 2/14 (40 queries, concurrency: 10, current batch size: 40)\n",
      "âœ… Batch 2 complete: 40 success, 0 failed, 27014 tokens, 69.4s\n",
      "ğŸ“ˆ Increasing batch size from 40 to 50 (success rate: 100.0%)\n",
      "ğŸ“ Progress saved: 2/14 batches complete (ETA: 0:12:08)\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch002_batch_002.json\n",
      "   Progress: 2/14 (14.3%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch002_batch_002.json\n",
      "ğŸ“Š Progress: 70/540 (13.0%)\n",
      "   Speed: 0.6 queries/sec | Success: 100.0%\n",
      "   ETA: 13m | Concurrency: 10 | Batch size: 50\n",
      "â³ Minimal delay: 0.1s (excellent performance: 100.0%)\n",
      "ğŸš€ Processing batch 3/12 (50 queries, concurrency: 10, current batch size: 50)\n",
      "âœ… Batch 3 complete: 50 success, 0 failed, 34001 tokens, 56.4s\n",
      "ğŸ“ˆ Increasing batch size from 50 to 60 (success rate: 100.0%)\n",
      "ğŸ“ Progress saved: 3/12 batches complete (ETA: 0:08:53)\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch003_batch_003.json\n",
      "   Progress: 3/12 (25.0%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch003_batch_003.json\n",
      "ğŸ“Š Progress: 120/540 (22.2%)\n",
      "   Speed: 0.7 queries/sec | Success: 100.0%\n",
      "   ETA: 10m | Concurrency: 10 | Batch size: 60\n",
      "â³ Minimal delay: 0.1s (excellent performance: 100.0%)\n",
      "ğŸš€ Processing batch 4/10 (60 queries, concurrency: 10, current batch size: 60)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 4 complete: 60 success, 0 failed, 40107 tokens, 92.6s\n",
      "ğŸ“ˆ Increasing batch size from 60 to 70 (success rate: 100.0%)\n",
      "ğŸš€ High success rate (100.0%) - increasing concurrency to 12\n",
      "ğŸ“ Progress saved: 4/10 batches complete (ETA: 0:06:45)\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch004_batch_004.json\n",
      "   Progress: 4/10 (40.0%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch004_batch_004.json\n",
      "ğŸ“Š Progress: 180/540 (33.3%)\n",
      "   Speed: 0.7 queries/sec | Success: 100.0%\n",
      "   ETA: 9m | Concurrency: 12 | Batch size: 70\n",
      "â³ Minimal delay: 0.1s (excellent performance: 100.0%)\n",
      "ğŸš€ Processing batch 5/10 (70 queries, concurrency: 12, current batch size: 70)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 5 complete: 70 success, 0 failed, 46501 tokens, 72.3s\n",
      "ğŸ“ˆ Increasing batch size from 70 to 80 (success rate: 100.0%)\n",
      "ğŸ“ Progress saved: 5/10 batches complete (ETA: 0:05:43)\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch005_batch_005.json\n",
      "   Progress: 5/10 (50.0%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch005_batch_005.json\n",
      "ğŸ“Š Progress: 250/540 (46.3%)\n",
      "   Speed: 0.7 queries/sec | Success: 100.0%\n",
      "   ETA: 6m | Concurrency: 12 | Batch size: 80\n",
      "â³ Minimal delay: 0.1s (excellent performance: 100.0%)\n",
      "ğŸš€ Processing batch 6/9 (80 queries, concurrency: 12, current batch size: 80)\n",
      "âœ… Batch 6 complete: 80 success, 0 failed, 54230 tokens, 77.1s\n",
      "ğŸ“ˆ Increasing batch size from 80 to 90 (success rate: 100.0%)\n",
      "ğŸ“ Progress saved: 6/9 batches complete (ETA: 0:03:30)\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch006_batch_006.json\n",
      "   Progress: 6/9 (66.7%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch006_batch_006.json\n",
      "ğŸ“Š Progress: 330/540 (61.1%)\n",
      "   Speed: 0.8 queries/sec | Success: 100.0%\n",
      "   ETA: 4m | Concurrency: 12 | Batch size: 90\n",
      "â³ Minimal delay: 0.1s (excellent performance: 100.0%)\n",
      "ğŸš€ Processing batch 7/9 (90 queries, concurrency: 12, current batch size: 90)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 7 complete: 90 success, 0 failed, 60453 tokens, 98.9s\n",
      "ğŸ“ˆ Increasing batch size from 90 to 100 (success rate: 100.0%)\n",
      "ğŸ“ Progress saved: 7/9 batches complete (ETA: 0:02:28)\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch007_batch_007.json\n",
      "   Progress: 7/9 (77.8%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch007_batch_007.json\n",
      "ğŸ“Š Progress: 420/540 (77.8%)\n",
      "   Speed: 0.8 queries/sec | Success: 100.0%\n",
      "   ETA: 2m | Concurrency: 12 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 100.0%)\n",
      "ğŸš€ Processing batch 8/9 (100 queries, concurrency: 12, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âœ… Batch 8 complete: 100 success, 0 failed, 66639 tokens, 102.7s\n",
      "ğŸš€ High success rate (100.0%) - increasing concurrency to 14\n",
      "ğŸ“ Progress saved: 8/9 batches complete (ETA: 0:01:17)\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch008_batch_008.json\n",
      "   Progress: 8/9 (88.9%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch008_batch_008.json\n",
      "ğŸ“Š Progress: 520/540 (96.3%)\n",
      "   Speed: 0.8 queries/sec | Success: 100.0%\n",
      "   ETA: 0m | Concurrency: 14 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 100.0%)\n",
      "ğŸš€ Processing batch 9/9 (20 queries, concurrency: 14, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âŒ Max retries reached, returning invalid response\n",
      "ğŸ”„ Added to retry queue: qwen/qwen3-next-80b-a3b-thinking_What are the health impacts of genetically modified food?_e7c0574639a244c8972c92aab9501035_rep20 (attempt 1/5)\n",
      "âœ… Batch 9 complete: 19 success, 1 failed, 11887 tokens, 43.3s\n",
      "ğŸ“ Progress saved: 9/9 batches complete (ETA: 0:00:00)\n",
      "ğŸ”„ Retry queue: 1 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch009_batch_009.json\n",
      "   Progress: 9/9 (100.0%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch009_batch_009.json\n",
      "ğŸ“Š Progress: 540/540 (100.0%)\n",
      "   Speed: 0.8 queries/sec | Success: 99.8%\n",
      "   ETA: 0m | Concurrency: 14 | Batch size: 100\n",
      "\n",
      "ğŸ”„ Processing retry queue to achieve 100% success...\n",
      "\n",
      "ğŸ”„ Retry round 1/5\n",
      "ğŸ”„ Processing 1 failed items with SMART retry logic...\n",
      "âœ… Retry round complete: 1 successful, 0 still failed\n",
      "   Retry success rate: 100.0%\n",
      "ğŸš€ High retry success rate - can be more aggressive in future\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch018_retry_round_1_010.json\n",
      "   Progress: 18/18 (100.0%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch018_retry_round_1_010.json\n",
      "ğŸ‰ All retries successful! 100% success achieved!\n",
      "\n",
      "ğŸ” Checking for missing task combinations...\n",
      "âœ… All expected task combinations are present!\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_225912_batch018_final_011.json\n",
      "   Progress: 18/18 (100.0%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225912_batch018_final_011.json\n",
      "\n",
      "ğŸ“Š Final Results:\n",
      "   Total queries processed: 541\n",
      "   Successful: 540 (99.8%)\n",
      "   Failed: 1 (0.2%)\n",
      "   \n",
      "   ğŸ“‹ Coverage Report:\n",
      "      Expected task combinations: 540\n",
      "      Completed task combinations: 540\n",
      "      Coverage: 100.0%\n",
      "   \n",
      "   Total tokens used: 362874\n",
      "   Total time: 669.9s (11.2 minutes)\n",
      "   Average speed: 0.8 queries/sec\n",
      "   Final concurrency: 14\n",
      "   Retry attempts: 1\n",
      "   Max retries reached: 0\n",
      "   Incremental files saved: 11 files in /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental\n",
      "ğŸ‰ PERFECT: 100% coverage with 99%+ success rate!\n",
      "\n",
      "âœ… Control experiment complete!\n",
      "   Results saved to: logs/control/\n",
      "\n",
      "\n",
      "âœ… Control experiment complete!\n",
      "   Results saved to: logs/control/\n",
      "   x-ai/grok-4: 20 responses\n",
      "   x-ai/grok-3: 20 responses\n",
      "   anthropic/claude-opus-4.1: 20 responses\n",
      "   anthropic/claude-sonnet-4.5: 20 responses\n",
      "   anthropic/claude-haiku-4.5: 20 responses\n",
      "   anthropic/claude-3.7-sonnet: 20 responses\n",
      "   anthropic/claude-3.5-haiku: 20 responses\n",
      "   google/gemini-2.5-pro: 20 responses\n",
      "   google/gemini-2.5-flash: 20 responses\n",
      "   google/gemma-3n-e4b-it: 20 responses\n",
      "   openai/gpt-5-chat-latest: 20 responses\n",
      "   openai/gpt-4o-2024-08-06: 20 responses\n",
      "   perplexity/sonar-pro-search: 20 responses\n",
      "   perplexity/sonar-deep-research: 20 responses\n",
      "   perplexity/sonar: 20 responses\n",
      "   mistralai/mistral-medium-3.1: 20 responses\n",
      "   mistralai/mistral-saba: 20 responses\n",
      "   mistralai/mistral-nemo: 20 responses\n",
      "   qwen/qwen3-max: 20 responses\n",
      "   qwen/qwen3-next-80b-a3b-thinking: 20 responses\n",
      "   qwen/qwen-turbo: 20 responses\n",
      "   deepseek/deepseek-v3.2-exp: 20 responses\n",
      "   deepseek/deepseek-v3.1-terminus: 20 responses\n",
      "   deepseek/deepseek-r1-0528: 20 responses\n",
      "   meta-llama/llama-4-maverick: 20 responses\n",
      "   meta-llama/llama-4-scout: 20 responses\n",
      "   meta-llama/llama-3.3-70b-instruct: 20 responses\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTROL EXPERIMENT: Measuring Within-Model Variance\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâš ï¸  This will run the same query 10 times for each of the 30 models\")\n",
    "print(\"   Total queries: 300\")\n",
    "print(\"   Estimated time: ~30 minutes\")\n",
    "print(\"\\nğŸ”„ Progress is saved incrementally to logs/control/\")\n",
    "print(\"\\nğŸš€ Starting control experiment...\\n\")\n",
    "\n",
    "control_results = run_control_experiment(\n",
    "    models=MODELS,\n",
    "    repetitions=CONTROL_REPETITIONS,\n",
    "    all_open_router=ALL_OPEN_ROUTER,\n",
    "    initial_batch_size=INITIAL_BATCH_SIZE,\n",
    "    initial_concurrency=INITIAL_CONCURRENCY,\n",
    "    max_concurrency=MAX_CONCURRENCY,\n",
    "    adaptive_mode=ADAPTIVE_MODE,\n",
    "    max_retries=5,\n",
    "    tag=CONTROL_TAG\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Control experiment complete!\")\n",
    "print(f\"   Results saved to: logs/control/\")\n",
    "for model in control_results:\n",
    "    total_responses = sum(len(personas) for personas in control_results[model].values())\n",
    "    print(f\"   {model}: {total_responses} responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Control Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Computing control similarities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities:   0%|          | 0/27 [00:00<?, ?topic/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence-transformers model from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:17<00:00,  1.55topic/s, current=meta-llama/llama-3.3-70b-instruct/What are the health impacts of genetically modified food?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Control similarities computed and saved!\n",
      "   Matrices computed: 27\n",
      "   Saved to: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/similarities_20251109_230947_control.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“Š Computing control similarities...\")\n",
    "\n",
    "control_matrices, control_dfs, control_personas, control_embeddings = \\\n",
    "    supercompute_similarities(control_results)\n",
    "\n",
    "# Save similarities\n",
    "control_sim_path = save_similarity_results(\n",
    "    control_matrices, control_dfs, control_personas, control_embeddings,\n",
    "    tag=CONTROL_TAG, subdir=\"control\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Control similarities computed and saved!\")\n",
    "print(f\"   Matrices computed: {sum(len(topics) for topics in control_matrices.values())}\")\n",
    "print(f\"   Saved to: {control_sim_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT: Load Cached Control Results\n",
    "\n",
    "**Skip control experiment** and load cached data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load cached control results\n",
    "# # Uncomment to use cached data:\n",
    "\n",
    "# control_results = load_latest_fast_results(subdir=\"control\")\n",
    "# if control_results:\n",
    "#     print(\"âœ… Loaded control results from cache!\")\n",
    "#     for model in control_results:\n",
    "#         total_responses = sum(len(personas) for personas in control_results[model].values())\n",
    "#         print(f\"   {model}: {total_responses} responses\")\n",
    "    \n",
    "#     # Load similarities\n",
    "#     control_sim_data = load_latest_similarity_results(subdir=\"control\")\n",
    "#     if control_sim_data:\n",
    "#         control_matrices, control_dfs, control_personas, control_embeddings = control_sim_data\n",
    "#         print(f\"\\nâœ… Loaded control similarities from cache!\")\n",
    "#     else:\n",
    "#         print(\"âš ï¸ No cached control similarities found. Run the cell above to compute.\")\n",
    "# else:\n",
    "#     print(\"âš ï¸ No cached control results found. Run the control experiment above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: MAIN EXPERIMENT\n",
    "\n",
    "## Measure Across-Persona Variance\n",
    "\n",
    "Query 100 diverse personas across 15 topics with 30 models.\n",
    "\n",
    "**Estimated time:** 6-12 hours for 45,000 queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MAIN EXPERIMENT: Measuring Across-Persona Variance\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Fetching 100 personas from NVIDIA Nemotron dataset...\n",
      "\n",
      "\n",
      "âœ… Loaded 100 personas\n",
      "   Saved to: logs/main/\n",
      "\n",
      "Sample Persona:\n",
      "   Age: 28, Sex: Female\n",
      "   Location: Madison, WI\n",
      "   Occupation: fast_food_or_counter_worker\n",
      "   Persona: Mary Alberti is a routineâ€‘obsessed, bulletâ€‘journal aficionado who balances disciplined work ambition...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MAIN EXPERIMENT: Measuring Across-Persona Variance\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“‹ Fetching {NUM_PERSONAS} personas from NVIDIA Nemotron dataset...\\n\")\n",
    "\n",
    "main_personas = get_and_clean_personas(\n",
    "    offset=0,\n",
    "    length=NUM_PERSONAS,\n",
    "    cache=True,\n",
    "    tag=MAIN_TAG,\n",
    "    subdir=\"main\"\n",
    ")\n",
    "\n",
    "num_personas = len(main_personas.get('rows', []))\n",
    "print(f\"\\nâœ… Loaded {num_personas} personas\")\n",
    "print(f\"   Saved to: logs/main/\")\n",
    "\n",
    "# Show sample\n",
    "if main_personas['rows']:\n",
    "    sample = main_personas['rows'][0]['row']\n",
    "    print(f\"\\nSample Persona:\")\n",
    "    print(f\"   Age: {sample.get('age')}, Sex: {sample.get('sex')}\")\n",
    "    print(f\"   Location: {sample.get('city')}, {sample.get('state')}\")\n",
    "    print(f\"   Occupation: {sample.get('occupation')}\")\n",
    "    print(f\"   Persona: {sample.get('persona', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT: Load Cached Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load cached personas\n",
    "# # Uncomment to use cached data:\n",
    "\n",
    "# main_personas = load_latest_personas(subdir=\"main\")\n",
    "# if main_personas:\n",
    "#     num_personas = len(main_personas.get('rows', []))\n",
    "#     print(f\"âœ… Loaded {num_personas} personas from cache\")\n",
    "# else:\n",
    "#     print(\"âš ï¸ No cached personas found. Run the cell above to fetch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Generating queries for 15 topics...\n",
      "\n",
      "âœ… Generated 1,500 queries per model\n",
      "   Total across all 27 models: 40,500\n",
      "\n",
      "   Breakdown:\n",
      "   - Are COVID-19 vaccines safe and effective?: 100 queries\n",
      "   - Is the U.S.-Mexico border secure?: 100 queries\n",
      "   - How has AI impacted employment rates?: 100 queries\n",
      "   ... and 12 more topics\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nğŸ”„ Generating queries for {len(TOPICS)} topics...\\n\")\n",
    "\n",
    "main_queries = generate_queries_for_personas(main_personas, TOPICS)\n",
    "\n",
    "total_queries_per_model = sum(len(topic_queries) for topic_queries in main_queries.values())\n",
    "print(f\"âœ… Generated {total_queries_per_model:,} queries per model\")\n",
    "print(f\"   Total across all {len(MODELS)} models: {total_queries_per_model * len(MODELS):,}\")\n",
    "print(f\"\\n   Breakdown:\")\n",
    "for topic in list(main_queries.keys())[:3]:  # Show first 3 topics\n",
    "    print(f\"   - {topic}: {len(main_queries[topic])} queries\")\n",
    "if len(main_queries) > 3:\n",
    "    print(f\"   ... and {len(main_queries) - 3} more topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Query LLMs\n",
    "\n",
    "**âš ï¸ WARNING: This will take 6-12 hours and make ~45,000 API calls!**\n",
    "\n",
    "Progress is saved incrementally to `logs/main/incremental/` - you can stop and resume anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERYING LLMs - MAIN EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  This will make 40,500 API calls\n",
      "   Estimated time: ~67.5 minutes (1.1 hours)\n",
      "\n",
      "ğŸ”„ Progress saved incrementally to: logs/main/incremental/\n",
      "   You can stop and resume anytime using query_llm_fast_resume()\n",
      "\n",
      "ğŸš€ Starting main experiment queries...\n",
      "\n",
      "ğŸ¯ Starting FAST robust query processing (100% success mode):\n",
      "   Models: 27\n",
      "   Topics: 15\n",
      "   Personas per topic: 100\n",
      "   Total queries: 40500\n",
      "   Initial batch size: 30\n",
      "   Initial concurrency: 10\n",
      "   Max concurrency: 500\n",
      "   Adaptive mode: True\n",
      "   All OpenRouter: True\n",
      "   Max retries: 5\n",
      "   100% success mode: True\n",
      "   Incremental saving: True\n",
      "   Incremental interval: Every 1 batch(es)\n",
      "   Incremental folder: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental\n",
      "ğŸ“‹ Created 40500 expected task combinations (modelÃ—topicÃ—persona)\n",
      "   Total batches: 1350\n",
      "ğŸš€ Processing batch 1/1350 (30 queries, concurrency: 10, current batch size: 30)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âŒ Max retries reached, returning invalid response\n",
      "ğŸ”„ Added to retry queue: qwen/qwen3-next-80b-a3b-thinking_Are COVID-19 vaccines safe and effective?_1 (attempt 1/5)\n",
      "âœ… Batch 1 complete: 29 success, 1 failed, 19468 tokens, 49.2s\n",
      "ğŸ“ˆ Increasing batch size from 30 to 40 (success rate: 96.7%)\n",
      "ğŸ“ Progress saved: 1/1350 batches complete (ETA: 18:25:39)\n",
      "ğŸ”„ Retry queue: 1 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch001_batch_001.json\n",
      "   Progress: 1/1350 (0.1%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch001_batch_001.json\n",
      "ğŸ“Š Progress: 30/40500 (0.1%)\n",
      "   Speed: 0.6 queries/sec | Success: 96.7%\n",
      "   ETA: 18h 25m | Concurrency: 10 | Batch size: 40\n",
      "â³ Short delay: 0.25s (good performance: 96.7%)\n",
      "ğŸš€ Processing batch 2/1013 (40 queries, concurrency: 10, current batch size: 40)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 2 complete: 40 success, 0 failed, 30336 tokens, 68.5s\n",
      "ğŸ“ˆ Increasing batch size from 40 to 50 (success rate: 100.0%)\n",
      "ğŸ“ Progress saved: 2/1013 batches complete (ETA: 16:33:19)\n",
      "ğŸ”„ Retry queue: 1 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch002_batch_002.json\n",
      "   Progress: 2/1013 (0.2%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch002_batch_002.json\n",
      "ğŸ“Š Progress: 70/40500 (0.2%)\n",
      "   Speed: 0.6 queries/sec | Success: 98.6%\n",
      "   ETA: 18h 55m | Concurrency: 10 | Batch size: 50\n",
      "â³ Minimal delay: 0.1s (excellent performance: 98.6%)\n",
      "ğŸš€ Processing batch 3/811 (50 queries, concurrency: 10, current batch size: 50)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âŒ Max retries reached, returning invalid response\n",
      "ğŸ”„ Added to retry queue: qwen/qwen3-next-80b-a3b-thinking_Are COVID-19 vaccines safe and effective?_3 (attempt 1/5)\n",
      "âœ… Batch 3 complete: 49 success, 1 failed, 37788 tokens, 59.4s\n",
      "ğŸ“ˆ Increasing batch size from 50 to 60 (success rate: 98.0%)\n",
      "ğŸ“ Progress saved: 3/811 batches complete (ETA: 13:16:16)\n",
      "ğŸ”„ Retry queue: 2 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch003_batch_003.json\n",
      "   Progress: 3/811 (0.4%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch003_batch_003.json\n",
      "ğŸ“Š Progress: 120/40500 (0.3%)\n",
      "   Speed: 0.7 queries/sec | Success: 98.3%\n",
      "   ETA: 16h 34m | Concurrency: 10 | Batch size: 60\n",
      "â³ Minimal delay: 0.1s (excellent performance: 98.3%)\n",
      "ğŸš€ Processing batch 4/676 (60 queries, concurrency: 10, current batch size: 60)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âŒ Max retries reached, returning invalid response\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "ğŸ”„ Added to retry queue: qwen/qwen3-next-80b-a3b-thinking_Are COVID-19 vaccines safe and effective?_5 (attempt 1/5)\n",
      "âœ… Batch 4 complete: 59 success, 1 failed, 42802 tokens, 86.7s\n",
      "ğŸ“ˆ Increasing batch size from 60 to 70 (success rate: 98.3%)\n",
      "ğŸš€ High success rate (98.3%) - increasing concurrency to 12\n",
      "ğŸ“ Progress saved: 4/676 batches complete (ETA: 12:19:45)\n",
      "ğŸ”„ Retry queue: 3 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch004_batch_004.json\n",
      "   Progress: 4/676 (0.6%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch004_batch_004.json\n",
      "ğŸ“Š Progress: 180/40500 (0.4%)\n",
      "   Speed: 0.7 queries/sec | Success: 98.3%\n",
      "   ETA: 16h 26m | Concurrency: 12 | Batch size: 70\n",
      "â³ Minimal delay: 0.1s (excellent performance: 98.3%)\n",
      "ğŸš€ Processing batch 5/580 (70 queries, concurrency: 12, current batch size: 70)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 5 complete: 70 success, 0 failed, 52796 tokens, 72.3s\n",
      "ğŸ“ˆ Increasing batch size from 70 to 80 (success rate: 100.0%)\n",
      "ğŸ“ Progress saved: 5/580 batches complete (ETA: 10:45:13)\n",
      "ğŸ”„ Retry queue: 3 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch005_batch_005.json\n",
      "   Progress: 5/580 (0.9%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch005_batch_005.json\n",
      "ğŸ“Š Progress: 250/40500 (0.6%)\n",
      "   Speed: 0.7 queries/sec | Success: 98.8%\n",
      "   ETA: 15h 3m | Concurrency: 12 | Batch size: 80\n",
      "â³ Minimal delay: 0.1s (excellent performance: 98.8%)\n",
      "ğŸš€ Processing batch 6/509 (80 queries, concurrency: 12, current batch size: 80)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âœ… Batch 6 complete: 80 success, 0 failed, 62360 tokens, 96.4s\n",
      "ğŸ“ˆ Increasing batch size from 80 to 90 (success rate: 100.0%)\n",
      "ğŸ“ Progress saved: 6/509 batches complete (ETA: 10:05:11)\n",
      "ğŸ”„ Retry queue: 3 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch006_batch_006.json\n",
      "   Progress: 6/509 (1.2%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch006_batch_006.json\n",
      "ğŸ“Š Progress: 330/40500 (0.8%)\n",
      "   Speed: 0.8 queries/sec | Success: 99.1%\n",
      "   ETA: 14h 38m | Concurrency: 12 | Batch size: 90\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.1%)\n",
      "ğŸš€ Processing batch 7/453 (90 queries, concurrency: 12, current batch size: 90)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âŒ Max retries reached, returning invalid response\n",
      "ğŸ”„ Added to retry queue: qwen/qwen3-next-80b-a3b-thinking_Are COVID-19 vaccines safe and effective?_14 (attempt 1/5)\n",
      "âœ… Batch 7 complete: 89 success, 1 failed, 63300 tokens, 104.5s\n",
      "ğŸ“ˆ Increasing batch size from 90 to 100 (success rate: 98.9%)\n",
      "ğŸ“ Progress saved: 7/453 batches complete (ETA: 9:31:01)\n",
      "ğŸ”„ Retry queue: 4 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch007_batch_007.json\n",
      "   Progress: 7/453 (1.5%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch007_batch_007.json\n",
      "ğŸ“Š Progress: 420/40500 (1.0%)\n",
      "   Speed: 0.8 queries/sec | Success: 99.0%\n",
      "   ETA: 14h 15m | Concurrency: 12 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.0%)\n",
      "ğŸš€ Processing batch 8/408 (100 queries, concurrency: 12, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 8 complete: 100 success, 0 failed, 73851 tokens, 88.7s\n",
      "ğŸš€ High success rate (100.0%) - increasing concurrency to 14\n",
      "ğŸ“ Progress saved: 8/408 batches complete (ETA: 8:42:05)\n",
      "ğŸ”„ Retry queue: 4 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch008_batch_008.json\n",
      "   Progress: 8/408 (2.0%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch008_batch_008.json\n",
      "ğŸ“Š Progress: 520/40500 (1.3%)\n",
      "   Speed: 0.8 queries/sec | Success: 99.2%\n",
      "   ETA: 13h 22m | Concurrency: 14 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.2%)\n",
      "ğŸš€ Processing batch 9/408 (100 queries, concurrency: 14, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âœ… Batch 9 complete: 100 success, 0 failed, 72214 tokens, 89.9s\n",
      "ğŸ“ Progress saved: 9/408 batches complete (ETA: 8:49:26)\n",
      "ğŸ”„ Retry queue: 4 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch009_batch_009.json\n",
      "   Progress: 9/408 (2.2%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch009_batch_009.json\n",
      "ğŸ“Š Progress: 620/40500 (1.5%)\n",
      "   Speed: 0.9 queries/sec | Success: 99.4%\n",
      "   ETA: 12h 48m | Concurrency: 14 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.4%)\n",
      "ğŸš€ Processing batch 10/408 (100 queries, concurrency: 14, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âŒ Max retries reached, returning invalid response\n",
      "ğŸ”„ Added to retry queue: qwen/qwen3-next-80b-a3b-thinking_Are COVID-19 vaccines safe and effective?_25 (attempt 1/5)\n",
      "âœ… Batch 10 complete: 99 success, 1 failed, 68291 tokens, 92.4s\n",
      "ğŸ“ Progress saved: 10/408 batches complete (ETA: 8:56:38)\n",
      "ğŸ”„ Retry queue: 5 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch010_batch_010.json\n",
      "   Progress: 10/408 (2.5%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch010_batch_010.json\n",
      "ğŸ“Š Progress: 720/40500 (1.8%)\n",
      "   Speed: 0.9 queries/sec | Success: 99.3%\n",
      "   ETA: 12h 24m | Concurrency: 14 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.3%)\n",
      "ğŸš€ Processing batch 11/408 (100 queries, concurrency: 14, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 11 complete: 100 success, 0 failed, 77445 tokens, 91.9s\n",
      "ğŸ“ Progress saved: 11/408 batches complete (ETA: 9:01:59)\n",
      "ğŸ”„ Retry queue: 5 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch011_batch_011.json\n",
      "   Progress: 11/408 (2.7%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch011_batch_011.json\n",
      "ğŸ“Š Progress: 820/40500 (2.0%)\n",
      "   Speed: 0.9 queries/sec | Success: 99.4%\n",
      "   ETA: 12h 6m | Concurrency: 14 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.4%)\n",
      "ğŸš€ Processing batch 12/408 (100 queries, concurrency: 14, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âœ… Batch 12 complete: 100 success, 0 failed, 74621 tokens, 88.1s\n",
      "ğŸš€ High success rate (100.0%) - increasing concurrency to 16\n",
      "ğŸ“ Progress saved: 12/408 batches complete (ETA: 9:04:06)\n",
      "ğŸ”„ Retry queue: 5 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch012_batch_012.json\n",
      "   Progress: 12/408 (2.9%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch012_batch_012.json\n",
      "ğŸ“Š Progress: 920/40500 (2.3%)\n",
      "   Speed: 0.9 queries/sec | Success: 99.5%\n",
      "   ETA: 11h 49m | Concurrency: 16 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.5%)\n",
      "ğŸš€ Processing batch 13/408 (100 queries, concurrency: 16, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 13 complete: 100 success, 0 failed, 75980 tokens, 84.2s\n",
      "ğŸ“ Progress saved: 13/408 batches complete (ETA: 9:03:39)\n",
      "ğŸ”„ Retry queue: 5 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch013_batch_013.json\n",
      "   Progress: 13/408 (3.2%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch013_batch_013.json\n",
      "ğŸ“Š Progress: 1020/40500 (2.5%)\n",
      "   Speed: 1.0 queries/sec | Success: 99.5%\n",
      "   ETA: 11h 32m | Concurrency: 16 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.5%)\n",
      "ğŸš€ Processing batch 14/408 (100 queries, concurrency: 16, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 14 complete: 100 success, 0 failed, 72462 tokens, 199.8s\n",
      "ğŸ“ Progress saved: 14/408 batches complete (ETA: 9:57:19)\n",
      "ğŸ”„ Retry queue: 5 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch014_batch_014.json\n",
      "   Progress: 14/408 (3.4%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch014_batch_014.json\n",
      "ğŸ“Š Progress: 1120/40500 (2.8%)\n",
      "   Speed: 0.9 queries/sec | Success: 99.6%\n",
      "   ETA: 12h 26m | Concurrency: 16 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.6%)\n",
      "ğŸš€ Processing batch 15/408 (100 queries, concurrency: 16, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âœ… Batch 15 complete: 100 success, 0 failed, 71363 tokens, 77.6s\n",
      "ğŸ“ Progress saved: 15/408 batches complete (ETA: 9:50:02)\n",
      "ğŸ”„ Retry queue: 5 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch015_batch_015.json\n",
      "   Progress: 15/408 (3.7%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch015_batch_015.json\n",
      "ğŸ“Š Progress: 1220/40500 (3.0%)\n",
      "   Speed: 0.9 queries/sec | Success: 99.6%\n",
      "   ETA: 12h 5m | Concurrency: 16 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.6%)\n",
      "ğŸš€ Processing batch 16/408 (100 queries, concurrency: 16, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âŒ Max retries reached, returning invalid response\n",
      "ğŸ”„ Added to retry queue: qwen/qwen3-next-80b-a3b-thinking_Are COVID-19 vaccines safe and effective?_46 (attempt 1/5)\n",
      "âœ… Batch 16 complete: 99 success, 1 failed, 71467 tokens, 83.3s\n",
      "ğŸš€ High success rate (99.0%) - increasing concurrency to 19\n",
      "ğŸ“ Progress saved: 16/408 batches complete (ETA: 9:45:49)\n",
      "ğŸ”„ Retry queue: 6 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch016_batch_016.json\n",
      "   Progress: 16/408 (3.9%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch016_batch_016.json\n",
      "ğŸ“Š Progress: 1320/40500 (3.3%)\n",
      "   Speed: 0.9 queries/sec | Success: 99.5%\n",
      "   ETA: 11h 49m | Concurrency: 19 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.5%)\n",
      "ğŸš€ Processing batch 17/408 (100 queries, concurrency: 19, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 17 complete: 100 success, 0 failed, 71768 tokens, 77.5s\n",
      "ğŸ“ Progress saved: 17/408 batches complete (ETA: 9:39:43)\n",
      "ğŸ”„ Retry queue: 6 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch017_batch_017.json\n",
      "   Progress: 17/408 (4.2%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch017_batch_017.json\n",
      "ğŸ“Š Progress: 1420/40500 (3.5%)\n",
      "   Speed: 0.9 queries/sec | Success: 99.6%\n",
      "   ETA: 11h 33m | Concurrency: 19 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.6%)\n",
      "ğŸš€ Processing batch 18/408 (100 queries, concurrency: 19, current batch size: 100)\n",
      "âœ… Batch 18 complete: 100 success, 0 failed, 73186 tokens, 71.2s\n",
      "ğŸ“ Progress saved: 18/408 batches complete (ETA: 9:31:53)\n",
      "ğŸ”„ Retry queue: 6 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch018_batch_018.json\n",
      "   Progress: 18/408 (4.4%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch018_batch_018.json\n",
      "ğŸ“Š Progress: 1520/40500 (3.8%)\n",
      "   Speed: 1.0 queries/sec | Success: 99.6%\n",
      "   ETA: 11h 16m | Concurrency: 19 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.6%)\n",
      "ğŸš€ Processing batch 19/408 (100 queries, concurrency: 19, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "âœ… Batch 19 complete: 100 success, 0 failed, 74325 tokens, 90.4s\n",
      "ğŸ“ Progress saved: 19/408 batches complete (ETA: 9:31:17)\n",
      "ğŸ”„ Retry queue: 6 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch019_batch_019.json\n",
      "   Progress: 19/408 (4.7%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch019_batch_019.json\n",
      "ğŸ“Š Progress: 1620/40500 (4.0%)\n",
      "   Speed: 1.0 queries/sec | Success: 99.6%\n",
      "   ETA: 11h 9m | Concurrency: 19 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.6%)\n",
      "ğŸš€ Processing batch 20/408 (100 queries, concurrency: 19, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âœ… Batch 20 complete: 100 success, 0 failed, 70624 tokens, 75.0s\n",
      "ğŸš€ High success rate (100.0%) - increasing concurrency to 22\n",
      "ğŸ“ Progress saved: 20/408 batches complete (ETA: 9:25:37)\n",
      "ğŸ”„ Retry queue: 6 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch020_batch_020.json\n",
      "   Progress: 20/408 (4.9%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch020_batch_020.json\n",
      "ğŸ“Š Progress: 1720/40500 (4.2%)\n",
      "   Speed: 1.0 queries/sec | Success: 99.7%\n",
      "   ETA: 10h 57m | Concurrency: 22 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.7%)\n",
      "ğŸš€ Processing batch 21/408 (100 queries, concurrency: 22, current batch size: 100)\n",
      "âœ… Batch 21 complete: 100 success, 0 failed, 75880 tokens, 65.4s\n",
      "ğŸ“ Progress saved: 21/408 batches complete (ETA: 9:17:25)\n",
      "ğŸ”„ Retry queue: 6 items pending\n",
      "ğŸ“ Incremental results saved: incremental_results_20251109_231038_batch021_batch_021.json\n",
      "   Progress: 21/408 (5.1%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/main/incremental/incremental_results_20251109_231038_batch021_batch_021.json\n",
      "ğŸ“Š Progress: 1820/40500 (4.5%)\n",
      "   Speed: 1.0 queries/sec | Success: 99.7%\n",
      "   ETA: 10h 42m | Concurrency: 22 | Batch size: 100\n",
      "â³ Minimal delay: 0.1s (excellent performance: 99.7%)\n",
      "ğŸš€ Processing batch 22/408 (100 queries, concurrency: 22, current batch size: 100)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "âš ï¸  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   You can stop and resume anytime using query_llm_fast_resume()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸš€ Starting main experiment queries...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m main_results = \u001b[43mquery_llm_fast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnested_queries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlist_of_models\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODELS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINITIAL_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINITIAL_CONCURRENCY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_CONCURRENCY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43madaptive_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mADAPTIVE_MODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_open_router\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALL_OPEN_ROUTER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_100_percent_success\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_incremental\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubdir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… Main experiment complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Results saved to: logs/main/\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Cyber/openai/whitehouse/consistencyAI/duplicity/fast_robust_queries.py:980\u001b[39m, in \u001b[36mquery_llm_fast\u001b[39m\u001b[34m(nested_queries, list_of_models, initial_batch_size, initial_concurrency, max_concurrency, adaptive_mode, all_open_router, max_retries, retry_delay, ensure_100_percent_success, save_incremental, incremental_interval, start_batch_number, initial_incremental_counter, subdir)\u001b[39m\n\u001b[32m    963\u001b[39m processor = FastRobustQueryProcessor(\n\u001b[32m    964\u001b[39m     initial_batch_size=initial_batch_size,\n\u001b[32m    965\u001b[39m     initial_concurrency=initial_concurrency,\n\u001b[32m   (...)\u001b[39m\u001b[32m    976\u001b[39m     subdir=subdir\n\u001b[32m    977\u001b[39m )\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_of_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_open_router\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    982\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Cyber/openai/whitehouse/consistencyAI/venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Cyber/openai/whitehouse/consistencyAI/venv/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Cyber/openai/whitehouse/consistencyAI/venv/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/selectors.py:561\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    559\u001b[39m ready = []\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m     kev_list = \u001b[38;5;28mself\u001b[39m._selector.control(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERYING LLMs - MAIN EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâš ï¸  This will make {total_queries_per_model * len(MODELS):,} API calls\")\n",
    "print(f\"   Estimated time: ~{total_queries_per_model * len(MODELS) / 10 / 60:.1f} minutes ({total_queries_per_model * len(MODELS) / 10 / 3600:.1f} hours)\")\n",
    "print(f\"\\nğŸ”„ Progress saved incrementally to: logs/main/incremental/\")\n",
    "print(\"   You can stop and resume anytime using query_llm_fast_resume()\")\n",
    "print(\"\\nğŸš€ Starting main experiment queries...\\n\")\n",
    "\n",
    "main_results = query_llm_fast(\n",
    "    nested_queries=main_queries,\n",
    "    list_of_models=MODELS,\n",
    "    initial_batch_size=INITIAL_BATCH_SIZE,\n",
    "    initial_concurrency=INITIAL_CONCURRENCY,\n",
    "    max_concurrency=MAX_CONCURRENCY,\n",
    "    adaptive_mode=ADAPTIVE_MODE,\n",
    "    all_open_router=ALL_OPEN_ROUTER,\n",
    "    max_retries=5,\n",
    "    ensure_100_percent_success=True,\n",
    "    save_incremental=True,\n",
    "    subdir=\"main\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Main experiment complete!\")\n",
    "print(f\"   Results saved to: logs/main/\")\n",
    "for model in list(main_results.keys())[:5]:  # Show first 5 models\n",
    "    total_responses = sum(len(personas) for personas in main_results[model].values())\n",
    "    print(f\"   {model}: {total_responses} responses\")\n",
    "if len(main_results) > 5:\n",
    "    print(f\"   ... and {len(main_results) - 5} more models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESUME: Continue Interrupted Main Experiment\n",
    "\n",
    "**Use this cell if the main experiment was interrupted and you need to resume.**\n",
    "\n",
    "This will:\n",
    "- Load progress from the most recent incremental save\n",
    "- Continue querying from where it left off\n",
    "- Ensure 100% completion"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### DIAGNOSTIC: Check for Partial Perplexity Data\n\n**Run this cell first to check if partial data exists for the removed models**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*80)\nprint(\"DIAGNOSTIC: Checking for Partial Perplexity Data\")\nprint(\"=\"*80)\n\nfrom duplicity import load_incremental_results\nfrom pathlib import Path\n\n# Models we removed from MODELS list\nREMOVED_MODELS = [\"perplexity/sonar-pro-search\", \"perplexity/sonar-deep-research\"]\n\n# Find the most recent incremental save file\nincremental_dir = \"logs/main/incremental\"\nincremental_files = sorted(\n    Path(incremental_dir).glob(\"incremental_*.json\"),\n    key=lambda p: p.stat().st_mtime,\n    reverse=True\n)\n\nif not incremental_files:\n    print(\"\\nâœ… No incremental save files found\")\n    print(\"   You can proceed with fresh start using cell 27\")\nelse:\n    latest_incremental = str(incremental_files[0])\n    print(f\"\\nğŸ“‚ Found incremental save: {latest_incremental}\")\n    \n    # Load and check for removed models\n    data = load_incremental_results(latest_incremental)\n    results = data.get('results', {})\n    \n    print(f\"\\nğŸ” Checking for removed Perplexity models...\\n\")\n    \n    found_partial_data = False\n    for model in REMOVED_MODELS:\n        if model in results:\n            found_partial_data = True\n            total_responses = sum(len(personas) for personas in results[model].values())\n            print(f\"âš ï¸  Found partial data for {model}:\")\n            print(f\"     {total_responses} responses\")\n            print(f\"     Topics covered: {len(results[model])} out of {len(TOPICS)}\")\n        else:\n            print(f\"âœ… No data for {model}\")\n    \n    if found_partial_data:\n        print(f\"\\nâš ï¸  PARTIAL DATA DETECTED!\")\n        print(f\"\\n   ğŸ“‹ RECOMMENDED ACTION:\")\n        print(f\"   Run the CLEANUP cell below to remove partial data from incremental file\")\n        print(f\"   This ensures analysis won't include incomplete data for removed models\")\n    else:\n        print(f\"\\nâœ… NO PARTIAL DATA - Safe to resume!\")\n        print(f\"   You can skip the cleanup cell and proceed to resume cell\")\n    \n    # Show overall progress\n    print(f\"\\nğŸ“Š Overall Progress:\")\n    total_models_in_save = len(results)\n    total_responses_in_save = sum(\n        len(personas)\n        for model_data in results.values()\n        for personas in model_data.values()\n    )\n    print(f\"   Models in save file: {total_models_in_save}\")\n    print(f\"   Total responses: {total_responses_in_save}\")\n    print(f\"   Expected with 27 models: {27 * 1500}\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### CLEANUP: Remove Partial Perplexity Data\n\n**Run this cell ONLY if the diagnostic above found partial data**\n\nThis will remove the two expensive Perplexity models from the incremental save file.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*80)\nprint(\"CLEANUP: Removing Partial Perplexity Data\")\nprint(\"=\"*80)\n\nimport json\nfrom duplicity import load_incremental_results\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Models to remove\nREMOVED_MODELS = [\"perplexity/sonar-pro-search\", \"perplexity/sonar-deep-research\"]\n\n# Find the most recent incremental save file\nincremental_dir = \"logs/main/incremental\"\nincremental_files = sorted(\n    Path(incremental_dir).glob(\"incremental_*.json\"),\n    key=lambda p: p.stat().st_mtime,\n    reverse=True\n)\n\nif not incremental_files:\n    print(\"\\nâš ï¸  No incremental save files found\")\n    print(\"   Nothing to clean up!\")\nelse:\n    latest_incremental = str(incremental_files[0])\n    print(f\"\\nğŸ“‚ Loading: {latest_incremental}\")\n    \n    # Load the data\n    with open(latest_incremental, 'r') as f:\n        data = json.load(f)\n    \n    results = data.get('results', {})\n    original_model_count = len(results)\n    \n    # Remove the unwanted models\n    removed_count = 0\n    removed_responses = 0\n    for model in REMOVED_MODELS:\n        if model in results:\n            model_responses = sum(len(personas) for personas in results[model].values())\n            removed_responses += model_responses\n            del results[model]\n            removed_count += 1\n            print(f\"\\nâŒ Removed {model}\")\n            print(f\"   ({model_responses} responses removed)\")\n    \n    if removed_count == 0:\n        print(\"\\nâœ… No models to remove - file is already clean!\")\n    else:\n        # Save the cleaned file\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        cleaned_path = f\"{incremental_dir}/incremental_results_{timestamp}_cleaned.json\"\n        \n        with open(cleaned_path, 'w') as f:\n            json.dump(data, f, indent=2)\n        \n        print(f\"\\nâœ… Cleanup complete!\")\n        print(f\"\\nğŸ“Š Summary:\")\n        print(f\"   Original models: {original_model_count}\")\n        print(f\"   Models removed: {removed_count}\")\n        print(f\"   Remaining models: {len(results)}\")\n        print(f\"   Responses removed: {removed_responses}\")\n        print(f\"\\nğŸ’¾ Cleaned file saved to:\")\n        print(f\"   {cleaned_path}\")\n        print(f\"\\nğŸ“‹ Next step:\")\n        print(f\"   Update the resume cell to use the cleaned file\")\n        print(f\"   Or let it auto-discover the most recent file (which is now cleaned)\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# RESUME: Uncomment and run this cell if you need to resume an interrupted experiment\n# (Instead of running the main query cell above)\n\nfrom duplicity import load_incremental_results\nfrom pathlib import Path\nimport os\n\n# Find the most recent incremental save file\nincremental_dir = \"logs/main/incremental\"\nincremental_files = sorted(\n    Path(incremental_dir).glob(\"incremental_*.json\"),\n    key=lambda p: p.stat().st_mtime,\n    reverse=True\n)\n\nif not incremental_files:\n    print(\"âš ï¸  No incremental save files found in logs/main/incremental/\")\n    print(\"   Run the main query cell above to start the experiment.\")\nelse:\n    latest_incremental = str(incremental_files[0])\n    print(f\"ğŸ“‚ Found incremental save: {latest_incremental}\")\n    \n    # Load partial results to see progress - FIX: extract 'results' key properly\n    data = load_incremental_results(latest_incremental)\n    partial_results = data.get('results', {})\n    total_expected = len(MODELS) * total_queries_per_model\n    completed = sum(\n        len(personas) \n        for model_data in partial_results.values() \n        for personas in model_data.values()\n    )\n    print(f\"ğŸ“Š Progress: {completed}/{total_expected} queries complete ({100*completed/total_expected:.1f}%)\")\n    print(f\"\\nğŸ”„ Resuming from: {latest_incremental}\\n\")\n    \n    # Resume the experiment\n    main_results = query_llm_fast_resume(\n        incremental_file_path=latest_incremental,\n        original_queries=main_queries,\n        list_of_models=MODELS,\n        initial_batch_size=INITIAL_BATCH_SIZE,\n        initial_concurrency=INITIAL_CONCURRENCY,\n        max_concurrency=MAX_CONCURRENCY,\n        adaptive_mode=ADAPTIVE_MODE,\n        all_open_router=ALL_OPEN_ROUTER,\n        max_retries=5,\n        ensure_100_percent_success=True,\n        save_incremental=True,\n        subdir=\"main\"\n    )\n    \n    print(\"\\nâœ… Resumed experiment complete!\")\n    print(f\"   Results saved to: logs/main/\")\n    for model in list(main_results.keys())[:5]:\n        total_responses = sum(len(personas) for personas in main_results[model].values())\n        print(f\"   {model}: {total_responses} responses\")\n    if len(main_results) > 5:\n        print(f\"   ... and {len(main_results) - 5} more models\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT: Load Cached Main Results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### FILTER: Remove Unwanted Models from Results\n\n**Run this cell after loading/resuming to ensure only current MODELS are included in analysis**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*80)\nprint(\"FILTER: Ensuring Only Current MODELS in Results\")\nprint(\"=\"*80)\n\n# Filter main_results to only include models in current MODELS list\nif 'main_results' in globals():\n    original_model_count = len(main_results)\n    \n    # Filter to only keep models that are in the current MODELS list\n    filtered_main_results = {\n        model: data \n        for model, data in main_results.items() \n        if model in MODELS\n    }\n    \n    removed_models = set(main_results.keys()) - set(filtered_main_results.keys())\n    \n    if removed_models:\n        print(f\"\\nâš ï¸  Filtered out {len(removed_models)} model(s) not in current MODELS list:\")\n        for model in removed_models:\n            total_responses = sum(len(personas) for personas in main_results[model].values())\n            print(f\"   - {model} ({total_responses} responses)\")\n        \n        # Replace main_results with filtered version\n        main_results = filtered_main_results\n        \n        print(f\"\\nâœ… Filtering complete!\")\n        print(f\"   Original models: {original_model_count}\")\n        print(f\"   Filtered models: {len(main_results)}\")\n        print(f\"   Expected models: {len(MODELS)}\")\n        \n        if len(main_results) == len(MODELS):\n            print(f\"\\nâœ… Perfect! main_results now contains exactly {len(MODELS)} models\")\n        else:\n            print(f\"\\nâš ï¸  Note: {len(MODELS) - len(main_results)} models from MODELS list not yet in results\")\n    else:\n        print(f\"\\nâœ… No filtering needed - all {original_model_count} models in results are in current MODELS list\")\nelse:\n    print(f\"\\nâš ï¸  Variable 'main_results' not found\")\n    print(f\"   Run one of the following cells first:\")\n    print(f\"   - Cell 27 (main query)\")\n    print(f\"   - Cell 33 (resume)\")\n    print(f\"   - Cell 36 (load cached)\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load cached main results\n",
    "# # Uncomment to use cached data:\n",
    "\n",
    "# main_results = load_latest_fast_results(subdir=\"main\")\n",
    "# if main_results:\n",
    "#     print(\"âœ… Loaded main results from cache!\")\n",
    "#     for model in list(main_results.keys())[:5]:\n",
    "#         total_responses = sum(len(personas) for personas in main_results[model].values())\n",
    "#         print(f\"   {model}: {total_responses} responses\")\n",
    "#     if len(main_results) > 5:\n",
    "#         print(f\"   ... and {len(main_results) - 5} more models\")\n",
    "# else:\n",
    "#     print(\"âš ï¸ No cached main results found. Run the query cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute Main Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Š Computing main experiment similarities...\")\n",
    "print(\"   This will compute embeddings and similarity matrices for all model/topic combinations\")\n",
    "print(\"   Estimated time: 5-10 minutes\\n\")\n",
    "\n",
    "main_matrices, main_dfs, main_personas_ids, main_embeddings = \\\n",
    "    supercompute_similarities(main_results)\n",
    "\n",
    "# Save similarities\n",
    "main_sim_path = save_similarity_results(\n",
    "    main_matrices, main_dfs, main_personas_ids, main_embeddings,\n",
    "    tag=MAIN_TAG, subdir=\"main\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Main similarities computed and saved!\")\n",
    "print(f\"   Matrices computed: {sum(len(topics) for topics in main_matrices.values())}\")\n",
    "print(f\"   Saved to: {main_sim_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT: Load Cached Main Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load cached main similarities\n",
    "# # Uncomment to use cached data:\n",
    "\n",
    "# main_sim_data = load_latest_similarity_results(subdir=\"main\")\n",
    "# if main_sim_data:\n",
    "#     main_matrices, main_dfs, main_personas_ids, main_embeddings = main_sim_data\n",
    "#     print(\"âœ… Loaded main similarities from cache!\")\n",
    "#     print(f\"   Matrices computed: {sum(len(topics) for topics in main_matrices.values())}\")\n",
    "# else:\n",
    "#     print(\"âš ï¸ No cached main similarities found. Run the cell above to compute.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Filter similarity matrices to only include current MODELS\n# This is important if loading cached similarities that might include removed models\n\nif 'main_matrices' in globals():\n    original_model_count = len(main_matrices)\n    \n    # Filter matrices to only keep models in current MODELS list\n    filtered_main_matrices = {\n        model: data \n        for model, data in main_matrices.items() \n        if model in MODELS\n    }\n    \n    # Also filter related data structures\n    if 'main_dfs' in globals():\n        main_dfs = {k: v for k, v in main_dfs.items() if k in MODELS}\n    \n    if 'main_personas_ids' in globals():\n        main_personas_ids = {k: v for k, v in main_personas_ids.items() if k in MODELS}\n    \n    if 'main_embeddings' in globals():\n        main_embeddings = {k: v for k, v in main_embeddings.items() if k in MODELS}\n    \n    removed_models = set(main_matrices.keys()) - set(filtered_main_matrices.keys())\n    \n    if removed_models:\n        print(f\"ğŸ§¹ Filtered out {len(removed_models)} model(s) from similarity matrices:\")\n        for model in removed_models:\n            print(f\"   - {model}\")\n        \n        main_matrices = filtered_main_matrices\n        print(f\"âœ… Matrices now contain {len(main_matrices)} models (expected: {len(MODELS)})\")\n    else:\n        print(f\"âœ… All {original_model_count} models in matrices are in current MODELS list\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: VARIANCE ANALYSIS\n",
    "\n",
    "## Compare Control vs. Persona Variance\n",
    "\n",
    "Analyze how models vary internally (control) vs. across personas (main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"output/variance_comparison\", exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE ANALYSIS: Control vs. Persona Variance\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸ“ˆ Computing within-model variance (control)...\")\n",
    "\n",
    "control_variance = compute_within_model_variance(\n",
    "    control_results,\n",
    "    control_matrices\n",
    ")\n",
    "\n",
    "control_variance.to_csv(\"output/variance_comparison/control_variance.csv\", index=False)\n",
    "print(f\"   Mean control similarity: {control_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Std dev: {control_variance['mean_similarity'].std():.4f}\")\n",
    "print(f\"   Saved to: output/variance_comparison/control_variance.csv\")\n",
    "\n",
    "print(\"\\nğŸ“‰ Computing across-persona variance (main)...\")\n",
    "\n",
    "persona_variance = compute_across_persona_variance(\n",
    "    main_results,\n",
    "    main_matrices\n",
    ")\n",
    "\n",
    "persona_variance.to_csv(\"output/variance_comparison/persona_variance.csv\", index=False)\n",
    "print(f\"   Mean persona similarity: {persona_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Std dev: {persona_variance['mean_similarity'].std():.4f}\")\n",
    "print(f\"   Saved to: output/variance_comparison/persona_variance.csv\")\n",
    "\n",
    "print(\"\\nâœ… Variance computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Š Creating variance comparison visualizations...\\n\")\n",
    "\n",
    "create_variance_comparison_visualizations(\n",
    "    control_variance,\n",
    "    persona_variance,\n",
    "    output_dir=\"output/variance_comparison\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Visualizations created!\")\n",
    "print(\"   Output files:\")\n",
    "print(\"   - comparison_bar_chart.png\")\n",
    "print(\"   - comparison_scatter.png\")\n",
    "print(\"   - comparison_heatmap.png\")\n",
    "print(\"   - variance_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Variance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“ Generating variance analysis report...\\n\")\n",
    "\n",
    "report = generate_variance_report(\n",
    "    control_variance,\n",
    "    persona_variance,\n",
    "    output_path=\"output/variance_comparison/report.txt\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸ“ All variance analysis files saved to: output/variance_comparison/\")\n",
    "print(\"\\nğŸ“Š Key Insights:\")\n",
    "print(f\"   Control mean similarity: {control_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Persona mean similarity: {persona_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Difference: {abs(control_variance['mean_similarity'].mean() - persona_variance['mean_similarity'].mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 4: STANDARD ANALYSES\n",
    "\n",
    "## Traditional ConsistencyAI visualizations and analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average scores from main experiment\n",
    "avg_scores = collect_avg_scores_by_model(main_matrices)\n",
    "\n",
    "print(\"\\nAverage Similarity Scores (Main Experiment):\")\n",
    "print(\"   (Higher = more consistent across personas)\\n\")\n",
    "\n",
    "for model in list(avg_scores.keys())[:5]:  # Show first 5\n",
    "    print(f\"\\n{model}:\")\n",
    "    for topic in list(avg_scores[model].keys())[:3]:  # Show first 3 topics\n",
    "        score = avg_scores[model][topic]\n",
    "        print(f\"   {topic}: {score:.4f}\")\n",
    "    if len(avg_scores[model]) > 3:\n",
    "        print(f\"   ... and {len(avg_scores[model]) - 3} more topics\")\n",
    "    overall = np.mean(list(avg_scores[model].values()))\n",
    "    print(f\"   Overall: {overall:.4f}\")\n",
    "\n",
    "if len(avg_scores) > 5:\n",
    "    print(f\"\\n... and {len(avg_scores) - 5} more models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Overall Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Š Creating overall leaderboard...\\n\")\n",
    "\n",
    "plot_overall_leaderboard(avg_scores, save_path=\"output\", show=True)\n",
    "\n",
    "print(\"\\nâœ… Leaderboard saved to: output/overall_leaderboard.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Topic-Specific Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Š Creating topic-specific plots...\\n\")\n",
    "\n",
    "plot_similarity_by_sphere(avg_scores, save_path=\"output\")\n",
    "\n",
    "print(\"\\nâœ… Topic plots saved to: output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Similarity Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Š Creating similarity heatmaps...\")\n",
    "print(\"   (This creates one heatmap per model/topic combination)\\n\")\n",
    "\n",
    "heatmap_count = 0\n",
    "for model in main_matrices:\n",
    "    for topic in main_matrices[model]:\n",
    "        safe_model = model.replace(\"/\", \"_\")\n",
    "        safe_topic = topic.replace(\" \", \"_\").replace(\"?\", \"\").replace(\"'\", \"\")\n",
    "        save_path = f\"output/heatmap_{safe_model}_{safe_topic}.png\"\n",
    "        \n",
    "        fig, ax = plot_similarity_matrix_with_values(\n",
    "            similarity_matrix=main_matrices[model][topic],\n",
    "            persona_ids=main_personas_ids[model][topic],\n",
    "            show_values=(len(main_personas_ids[model][topic]) <= 20),\n",
    "            model_name=model,\n",
    "            topic=topic,\n",
    "            save_path=save_path\n",
    "        )\n",
    "        plt.close(fig)\n",
    "        heatmap_count += 1\n",
    "\n",
    "print(f\"\\nâœ… Created {heatmap_count} heatmaps in: output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Š Performing clustering analysis...\\n\")\n",
    "\n",
    "analysis_results = analyze_and_cluster_embeddings(\n",
    "    all_embeddings=main_embeddings,\n",
    "    all_similarity_matrices=main_matrices,\n",
    "    all_sorted_personas=main_personas_ids,\n",
    "    max_clusters=10,\n",
    "    random_state=42,\n",
    "    save_plots=True,\n",
    "    plots_dir=\"output/clustering\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Clustering analysis complete!\\n\")\n",
    "\n",
    "# Print summary\n",
    "print_analysis_summary(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Š Running central analysis...\\n\")\n",
    "\n",
    "per_model_per_topic, model_overall_weighted, topic_across_models_weighted, benchmark = \\\n",
    "    compute_central_analysis(main_matrices, output_dir=\"output/analysis\")\n",
    "\n",
    "print(\"\\nâœ… Central analysis complete!\\n\")\n",
    "\n",
    "# Display results\n",
    "print_central_analysis_summary(model_overall_weighted, topic_across_models_weighted, benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## VERIFICATION: Confirm Only 27 Models in Outputs\n\n**Run this cell to verify that all outputs only contain the expected 27 models**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*80)\nprint(\"VERIFICATION: Checking Model Counts in All Outputs\")\nprint(\"=\"*80)\n\nexpected_models = len(MODELS)\nremoved_models_list = [\"perplexity/sonar-pro-search\", \"perplexity/sonar-deep-research\"]\n\nprint(f\"\\nğŸ“‹ Expected models in current MODELS list: {expected_models}\")\nprint(f\"ğŸ“‹ Removed models: {', '.join(removed_models_list)}\")\n\nissues = []\n\n# Check main_results\nif 'main_results' in globals():\n    result_models = len(main_results)\n    print(f\"\\nâœ“ main_results: {result_models} models\")\n    if result_models != expected_models:\n        issues.append(f\"main_results has {result_models} models, expected {expected_models}\")\n    # Check for removed models\n    for removed in removed_models_list:\n        if removed in main_results:\n            issues.append(f\"Found removed model '{removed}' in main_results\")\nelse:\n    print(f\"\\nâš ï¸  main_results not found (not run yet)\")\n\n# Check main_matrices\nif 'main_matrices' in globals():\n    matrix_models = len(main_matrices)\n    print(f\"âœ“ main_matrices: {matrix_models} models\")\n    if matrix_models != expected_models:\n        issues.append(f\"main_matrices has {matrix_models} models, expected {expected_models}\")\n    # Check for removed models\n    for removed in removed_models_list:\n        if removed in main_matrices:\n            issues.append(f\"Found removed model '{removed}' in main_matrices\")\nelse:\n    print(f\"âš ï¸  main_matrices not found (not computed yet)\")\n\n# Check variance CSVs\nimport pandas as pd\nvariance_files = [\n    \"output/variance_comparison/control_variance.csv\",\n    \"output/variance_comparison/persona_variance.csv\"\n]\n\nfor file in variance_files:\n    try:\n        df = pd.read_csv(file)\n        unique_models = df['model'].nunique()\n        print(f\"âœ“ {file.split('/')[-1]}: {unique_models} models\")\n        if unique_models != expected_models:\n            issues.append(f\"{file} has {unique_models} models, expected {expected_models}\")\n        # Check for removed models\n        for removed in removed_models_list:\n            if removed in df['model'].values:\n                issues.append(f\"Found removed model '{removed}' in {file}\")\n    except FileNotFoundError:\n        print(f\"âš ï¸  {file.split('/')[-1]}: not found (not created yet)\")\n    except Exception as e:\n        print(f\"âš ï¸  {file.split('/')[-1]}: error reading - {e}\")\n\n# Check analysis CSVs\ntry:\n    analysis_file = \"output/analysis/model_overall_weighted.csv\"\n    df = pd.read_csv(analysis_file)\n    unique_models = len(df)\n    print(f\"âœ“ model_overall_weighted.csv: {unique_models} models\")\n    if unique_models != expected_models:\n        issues.append(f\"model_overall_weighted.csv has {unique_models} models, expected {expected_models}\")\n    # Check for removed models\n    for removed in removed_models_list:\n        if removed in df['model'].values:\n            issues.append(f\"Found removed model '{removed}' in model_overall_weighted.csv\")\nexcept FileNotFoundError:\n    print(f\"âš ï¸  model_overall_weighted.csv: not found (not created yet)\")\nexcept Exception as e:\n    print(f\"âš ï¸  model_overall_weighted.csv: error reading - {e}\")\n\n# Summary\nprint(\"\\n\" + \"=\"*80)\nif issues:\n    print(\"âŒ VERIFICATION FAILED!\")\n    print(f\"\\n   Found {len(issues)} issue(s):\\n\")\n    for issue in issues:\n        print(f\"   - {issue}\")\n    print(f\"\\n   Please re-run the FILTER cells to fix these issues\")\nelse:\n    print(\"âœ… VERIFICATION PASSED!\")\n    print(f\"\\n   All outputs contain exactly {expected_models} models\")\n    print(f\"   No removed Perplexity models found\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EXPERIMENT COMPLETE!\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "### What you just did:\n",
    "\n",
    "**Control Experiment:**\n",
    "- Measured within-model variance using Mary Alberti persona\n",
    "- Ran same query 10 times per model (300 total queries)\n",
    "- Identified most internally consistent models\n",
    "\n",
    "**Main Experiment:**\n",
    "- Loaded 100 diverse personas from NVIDIA Nemotron\n",
    "- Generated personalized queries across 15 sensitive topics\n",
    "- Queried 30 LLMs (45,000 total queries)\n",
    "- Measured across-persona variance\n",
    "\n",
    "**Variance Analysis:**\n",
    "- Compared control vs. persona variance\n",
    "- Identified most persona-sensitive models\n",
    "- Generated comprehensive visualizations and reports\n",
    "\n",
    "**Standard Analysis:**\n",
    "- Created similarity heatmaps\n",
    "- Generated leaderboards and topic plots\n",
    "- Performed clustering analysis\n",
    "- Computed central analysis metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Output Files\n",
    "\n",
    "### Control Experiment (`logs/control/`):\n",
    "- Query results and similarities\n",
    "- Incremental progress files\n",
    "\n",
    "### Main Experiment (`logs/main/`):\n",
    "- Persona data (100 personas)\n",
    "- Query results (45,000 responses)\n",
    "- Similarity matrices and embeddings\n",
    "- Incremental progress files\n",
    "\n",
    "### Variance Analysis (`output/variance_comparison/`):\n",
    "- `control_variance.csv` - Within-model variance stats\n",
    "- `persona_variance.csv` - Across-persona variance stats\n",
    "- `comparison_bar_chart.png` - Side-by-side comparison\n",
    "- `comparison_scatter.png` - Control vs. persona scatter\n",
    "- `comparison_heatmap.png` - Metrics heatmap\n",
    "- `variance_distributions.png` - Distribution box plots\n",
    "- `report.txt` - Detailed text report\n",
    "\n",
    "### Standard Analysis (`output/`):\n",
    "- Similarity heatmaps (one per model/topic)\n",
    "- Overall leaderboard\n",
    "- Topic-specific plots\n",
    "- Clustering visualizations (`output/clustering/`)\n",
    "- Central analysis CSVs (`output/analysis/`)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "**Understanding Variance:**\n",
    "- **High control variance** = Model is noisy/inconsistent with itself\n",
    "- **Low control variance** = Model is stable and consistent\n",
    "- **High persona variance** = Model adapts responses to different personas\n",
    "- **Low persona variance** = Model gives similar responses regardless of persona\n",
    "\n",
    "**Ideal Model Profile:**\n",
    "- âœ… High control similarity (consistent with itself)\n",
    "- âœ… Lower persona similarity (persona-aware)\n",
    "- = Stable behavior that adapts to demographic differences\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review variance comparison visualizations in `output/variance_comparison/`\n",
    "2. Read the detailed report: `output/variance_comparison/report.txt`\n",
    "3. Examine model-specific heatmaps in `output/`\n",
    "4. Analyze central analysis results in `output/analysis/`\n",
    "5. Share your findings!\n",
    "\n",
    "---\n",
    "\n",
    "**Built by the Duke Phishermen**  \n",
    "Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute\n",
    "\n",
    "With inquiries, questions, & feedback, please contact: peter dot banyas at duke dot edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}