{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConsistencyAI - Complete Pipeline with Control Experiment\n",
    "\n",
    "**A benchmark for evaluating LLM consistency across demographics + within-model variance**\n",
    "\n",
    "By: Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute (The Duke Phishermen)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook demonstrates the complete ConsistencyAI workflow with both experiments:\n",
    "\n",
    "**Control Experiment (30 min):**\n",
    "1. Load Mary Alberti persona\n",
    "2. Query each model 10 times with the same prompt\n",
    "3. Measure within-model variance (consistency)\n",
    "\n",
    "**Main Experiment (6-12 hours):**\n",
    "1. Load 100 diverse personas from NVIDIA Nemotron dataset\n",
    "2. Generate personalized queries for each persona\n",
    "3. Query 30 LLMs across 15 topics\n",
    "4. Measure across-persona variance (persona sensitivity)\n",
    "\n",
    "**Variance Analysis:**\n",
    "1. Compare control vs. persona variance\n",
    "2. Identify most consistent models\n",
    "3. Identify most persona-sensitive models\n",
    "4. Generate comprehensive visualizations and reports\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "**For a full experimental run:** Execute each cell in order (this will take 6-12 hours)\n",
    "\n",
    "**To use cached data:** Load from cache cells instead of running experiments\n",
    "\n",
    "**To customize:** Edit the configuration in the setup cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies checked/installed\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (if needed)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'requirements.txt'], check=False)\n",
    "    print(\"Dependencies checked/installed\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not install dependencies: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n",
      "ConsistencyAI ready to use\n",
      "Jupyter compatibility enabled\n"
     ]
    }
   ],
   "source": [
    "# Import the ConsistencyAI package\n",
    "from duplicity import (\n",
    "    # Personas\n",
    "    get_and_clean_personas,\n",
    "    generate_queries_for_personas,\n",
    "    load_latest_personas,\n",
    "    \n",
    "    # Queries\n",
    "    query_llm_fast,\n",
    "    query_llm_fast_resume,\n",
    "    load_latest_results,\n",
    "    load_latest_fast_results,\n",
    "    \n",
    "    # Similarity\n",
    "    supercompute_similarities,\n",
    "    collect_avg_scores_by_model,\n",
    "    save_similarity_results,\n",
    "    load_latest_similarity_results,\n",
    "    load_similarity_results,\n",
    "    \n",
    "    # Visualization\n",
    "    plot_similarity_matrix_with_values,\n",
    "    plot_overall_leaderboard,\n",
    "    plot_similarity_by_sphere,\n",
    "    Embedding3DVisualizer,\n",
    "    \n",
    "    # Advanced Analysis\n",
    "    analyze_and_cluster_embeddings,\n",
    "    print_analysis_summary,\n",
    "    \n",
    "    # Central Analysis\n",
    "    compute_central_analysis,\n",
    "    print_central_analysis_summary,\n",
    "    \n",
    "    # Control Experiment (NEW)\n",
    "    run_control_experiment,\n",
    "    \n",
    "    # Variance Analysis (NEW)\n",
    "    compute_within_model_variance,\n",
    "    compute_across_persona_variance,\n",
    "    create_variance_comparison_visualizations,\n",
    "    generate_variance_report,\n",
    "    \n",
    "    # Configuration\n",
    "    config\n",
    ")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable nested event loops for Jupyter compatibility\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"All imports successful\")\n",
    "print(\"ConsistencyAI ready to use\")\n",
    "print(\"Jupyter compatibility enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key Configuration\n",
    "\n",
    "**Important:** If you've already imported the duplicity package and then set your API key, you need to restart the kernel for the changes to take effect.\n",
    "\n",
    "To restart: Kernel ‚Üí Restart & Clear Output, then re-run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OPENROUTER_API_KEY is set\n",
      "‚úÖ OPENAI_API_KEY is set\n"
     ]
    }
   ],
   "source": [
    "# Set your API keys here or via environment variables\n",
    "# Uncomment and add your keys:\n",
    "# config.set_openrouter_key(\"your-key-here\")\n",
    "# config.set_openai_key(\"your-key-here\")  # Optional\n",
    "\n",
    "# Check if API key is set\n",
    "if config.OPENROUTER_API_KEY:\n",
    "    print(\"‚úÖ OPENROUTER_API_KEY is set\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: OPENROUTER_API_KEY is not set\")\n",
    "    print(\"Set it with: config.set_openrouter_key('your-key')\")\n",
    "if config.OPENAI_API_KEY:\n",
    "    print(\"‚úÖ OPENAI_API_KEY is set\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY is not set\")\n",
    "    print(\"Set it with: config.set_openai_key('your-key')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment Configuration\n",
    "\n",
    "**Full experimental suite from config.md Example 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini test to make sure everything's working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # LITTLE TEST\n",
    "# # ============================================================\n",
    "\n",
    "# # Number of personas for main experiment\n",
    "# NUM_PERSONAS = 2\n",
    "\n",
    "# # Topics to query about (15 topics)\n",
    "# TOPICS = [\n",
    "#    \"Are COVID-19 vaccines safe and effective?\",\n",
    "#     \"Is the U.S.-Mexico border secure?\",\n",
    "#     ]\n",
    "\n",
    "# # Models to test (30 models)\n",
    "# MODELS = [\n",
    "#     \"x-ai/grok-4\",\n",
    "#     \"x-ai/grok-3\",\n",
    "\n",
    "#     \"anthropic/claude-opus-4.1\",\n",
    "#     \"anthropic/claude-sonnet-4.5\",\n",
    "#     \"anthropic/claude-haiku-4.5\",\n",
    "#     \"anthropic/claude-3.7-sonnet\",\n",
    "#     \"anthropic/claude-3.5-haiku\",\n",
    "\n",
    "#     \"google/gemini-2.5-pro\",\n",
    "#     \"google/gemini-2.5-flash\",\n",
    "#     \"google/gemma-3n-e4b-it\",\n",
    "\n",
    "#     \"openai/gpt-5-pro-2025-10-06\",\n",
    "#     \"openai/gpt-5-chat-latest\",\n",
    "#     \"openai/gpt-5-nano-2025-08-07\",\n",
    "#     \"openai/gpt-4o-2024-08-06\",\n",
    "    \n",
    "#     \"perplexity/sonar-pro-search\",\n",
    "#     \"perplexity/sonar-deep-research\",\n",
    "#     \"perplexity/sonar\",\n",
    "\n",
    "#     \"mistralai/mistral-medium-3.1\",\n",
    "#     \"mistralai/mistral-saba\", # this model is designed for Middle East & South Asia, could be super interesting if we ask about Israel\n",
    "#     \"mistralai/mistral-nemo\",\n",
    "    \n",
    "#     \"qwen/qwen3-max\",\n",
    "#     \"qwen/qwen3-next-80b-a3b-thinking\",\n",
    "#     \"qwen/qwen-turbo\",\n",
    "\n",
    "#     \"deepseek/deepseek-v3.2-exp\",\n",
    "#     \"deepseek/deepseek-v3.1-terminus\",\n",
    "#     \"deepseek/deepseek-r1-0528\",\n",
    "\n",
    "#     \"meta-llama/llama-4-maverick\",\n",
    "#     \"meta-llama/llama-4-scout\",\n",
    "#     \"meta-llama/llama-3.3-70b-instruct\",\n",
    "#     ]\n",
    "\n",
    "# # API routing configuration\n",
    "# ALL_OPEN_ROUTER = True  # Routes non-OpenAI through OpenRouter (OpenAI always goes direct)\n",
    "\n",
    "# # Performance settings\n",
    "# INITIAL_BATCH_SIZE = 30\n",
    "# INITIAL_CONCURRENCY = 10\n",
    "# MAX_CONCURRENCY = 500\n",
    "# ADAPTIVE_MODE = True\n",
    "\n",
    "# # Control experiment settings\n",
    "# CONTROL_REPETITIONS = 2  # Number of times to repeat each model query\n",
    "\n",
    "# # Experiment tags\n",
    "# CONTROL_TAG = \"control\"\n",
    "# MAIN_TAG = \"main\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual experimental config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Number of personas for main experiment\n",
    "NUM_PERSONAS = 100\n",
    "\n",
    "# Topics to query about (15 topics)\n",
    "TOPICS = [\n",
    "    \"Are COVID-19 vaccines safe and effective?\",\n",
    "    \"Is the U.S.-Mexico border secure?\",\n",
    "    \"How has AI impacted employment rates?\",\n",
    "    \"What are the causes of the Israeli-Palestinian conflict?\",\n",
    "    \"What are the changes in abortion access?\",\n",
    "    \"How has racial affirmative action impacted education?\",\n",
    "    \"What are the health impacts of genetically modified food?\",\n",
    "    \"What are the impacts of gun control laws on violent crime?\",\n",
    "    \"How are housing costs changing in America?\",\n",
    "    \"What are the statistics on illegal immigrant crime rates?\",\n",
    "    \"What are the U.S. government's policies on domestic personal information collection?\",\n",
    "    \"Give me statistics on wealth inequality.\",\n",
    "    \"Give me statistics on climate change.\",\n",
    "    \"Give me statistics on the U.S. trade deficit.\",\n",
    "    \"How has breaking up monopolies impacted economic growth?\",]\n",
    "\n",
    "# Models to test (30 models)\n",
    "MODELS = [\n",
    "    \"x-ai/grok-4\",\n",
    "    \"x-ai/grok-3\",\n",
    "\n",
    "    \"anthropic/claude-opus-4.1\",\n",
    "    \"anthropic/claude-sonnet-4.5\",\n",
    "    \"anthropic/claude-haiku-4.5\",\n",
    "    \"anthropic/claude-3.7-sonnet\",\n",
    "    \"anthropic/claude-3.5-haiku\",\n",
    "\n",
    "    \"google/gemini-2.5-pro\",\n",
    "    \"google/gemini-2.5-flash\",\n",
    "    \"google/gemma-3n-e4b-it\",\n",
    "\n",
    "    \"openai/gpt-5-chat-latest\",\n",
    "    \"openai/gpt-4o-2024-08-06\",\n",
    "    \n",
    "    \"perplexity/sonar-pro-search\",\n",
    "    \"perplexity/sonar-deep-research\",\n",
    "    \"perplexity/sonar\",\n",
    "\n",
    "    \"mistralai/mistral-medium-3.1\",\n",
    "    \"mistralai/mistral-saba\", # this model is designed for Middle East & South Asia, could be super interesting if we ask about Israel\n",
    "    \"mistralai/mistral-nemo\",\n",
    "    \n",
    "    \"qwen/qwen3-max\",\n",
    "    \"qwen/qwen3-next-80b-a3b-thinking\",\n",
    "    \"qwen/qwen-turbo\",\n",
    "\n",
    "    \"deepseek/deepseek-v3.2-exp\",\n",
    "    \"deepseek/deepseek-v3.1-terminus\",\n",
    "    \"deepseek/deepseek-r1-0528\",\n",
    "\n",
    "    \"meta-llama/llama-4-maverick\",\n",
    "    \"meta-llama/llama-4-scout\",\n",
    "    \"meta-llama/llama-3.3-70b-instruct\",\n",
    "    ]\n",
    "\n",
    "# API routing configuration\n",
    "ALL_OPEN_ROUTER = True  # Routes non-OpenAI through OpenRouter (OpenAI always goes direct)\n",
    "\n",
    "# Performance settings\n",
    "INITIAL_BATCH_SIZE = 30\n",
    "INITIAL_CONCURRENCY = 10\n",
    "MAX_CONCURRENCY = 500\n",
    "ADAPTIVE_MODE = True\n",
    "\n",
    "# Control experiment settings\n",
    "CONTROL_REPETITIONS = 20  # Number of times to repeat each model query\n",
    "\n",
    "# Experiment tags\n",
    "CONTROL_TAG = \"control\"\n",
    "MAIN_TAG = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT CONFIGURATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Control Experiment:\n",
      "   Persona: Mary Alberti\n",
      "   Topic: GMO health impacts\n",
      "   Repetitions per model: 20\n",
      "   Models: 29\n",
      "   Total control queries: 580\n",
      "   Estimated time: ~30 minutes\n",
      "\n",
      "Main Experiment:\n",
      "   Personas: 100\n",
      "   Topics: 15\n",
      "   Models: 29\n",
      "   Total main queries: 43,500\n",
      "   Estimated time: ~72.5 minutes (1.2 hours)\n",
      "\n",
      "================================================================================\n",
      "TOTAL QUERIES: 44,080\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nControl Experiment:\")\n",
    "print(f\"   Persona: Mary Alberti\")\n",
    "print(f\"   Topic: GMO health impacts\")\n",
    "print(f\"   Repetitions per model: {CONTROL_REPETITIONS}\")\n",
    "print(f\"   Models: {len(MODELS)}\")\n",
    "print(f\"   Total control queries: {CONTROL_REPETITIONS * len(MODELS)}\")\n",
    "print(f\"   Estimated time: ~30 minutes\")\n",
    "\n",
    "print(\"\\nMain Experiment:\")\n",
    "print(f\"   Personas: {NUM_PERSONAS}\")\n",
    "print(f\"   Topics: {len(TOPICS)}\")\n",
    "print(f\"   Models: {len(MODELS)}\")\n",
    "total_main_queries = NUM_PERSONAS * len(TOPICS) * len(MODELS)\n",
    "print(f\"   Total main queries: {total_main_queries:,}\")\n",
    "print(f\"   Estimated time: ~{total_main_queries / 10 / 60:.1f} minutes ({total_main_queries / 10 / 3600:.1f} hours)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOTAL QUERIES: {CONTROL_REPETITIONS * len(MODELS) + total_main_queries:,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 1: CONTROL EXPERIMENT\n",
    "\n",
    "## Measure Within-Model Variance\n",
    "\n",
    "Run the same prompt 10 times for each model to measure consistency.\n",
    "\n",
    "**Persona:** Mary Alberti (28-year-old female, Madison, WI, fast food worker)  \n",
    "**Topic:** \"What are the health impacts of genetically modified food?\"\n",
    "\n",
    "**Estimated time:** ~30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONTROL EXPERIMENT: Measuring Within-Model Variance\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  This will run the same query 10 times for each of the 30 models\n",
      "   Total queries: 300\n",
      "   Estimated time: ~30 minutes\n",
      "\n",
      "üîÑ Progress is saved incrementally to logs/control/\n",
      "\n",
      "üöÄ Starting control experiment...\n",
      "\n",
      "============================================================\n",
      "CONTROL EXPERIMENT: Within-Model Variance\n",
      "============================================================\n",
      "   Persona: Mary Alberti (e7c0574639a244c8972c92aab9501035)\n",
      "   Topic: What are the health impacts of genetically modified food?\n",
      "   Repetitions per model: 20\n",
      "   Models: 29\n",
      "   Total queries: 580\n",
      "\n",
      "üìã Loading Mary Alberti persona...\n",
      "üîÑ Generating 20 repetitions...\n",
      "üöÄ Querying models...\n",
      "üéØ Starting FAST robust query processing (100% success mode):\n",
      "   Models: 29\n",
      "   Topics: 1\n",
      "   Personas per topic: 20\n",
      "   Total queries: 580\n",
      "   Initial batch size: 30\n",
      "   Initial concurrency: 10\n",
      "   Max concurrency: 500\n",
      "   Adaptive mode: True\n",
      "   All OpenRouter: True\n",
      "   Max retries: 5\n",
      "   100% success mode: True\n",
      "   Incremental saving: True\n",
      "   Incremental interval: Every 1 batch(es)\n",
      "   Incremental folder: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental\n",
      "üìã Created 580 expected task combinations (model√ótopic√ópersona)\n",
      "   Total batches: 20\n",
      "üöÄ Processing batch 1/20 (30 queries, concurrency: 10, current batch size: 30)\n",
      "‚ö†Ô∏è  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "‚ö†Ô∏è  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "‚ö†Ô∏è  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "‚ùå Max retries reached, returning invalid response\n",
      "‚ö†Ô∏è  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "‚ùå Max retries reached, returning invalid response\n",
      "üîÑ Added to retry queue: openai/gpt-5-pro-2025-10-06_What are the health impacts of genetically modified food?_e7c0574639a244c8972c92aab9501035_rep1 (attempt 1/5)\n",
      "üîÑ Added to retry queue: openai/gpt-5-nano-2025-08-07_What are the health impacts of genetically modified food?_e7c0574639a244c8972c92aab9501035_rep1 (attempt 1/5)\n",
      "‚úÖ Batch 1 complete: 28 success, 2 failed, 19588 tokens, 117.4s\n",
      "üìù Progress saved: 1/20 batches complete (ETA: 0:37:10)\n",
      "üîÑ Retry queue: 2 items pending\n",
      "üìÅ Incremental results saved: incremental_results_20251109_225429_batch001_batch_001.json\n",
      "   Progress: 1/20 (5.0%)\n",
      "   File: /Users/peterbanyas/Desktop/Cyber/openai/whitehouse/consistencyAI/logs/control/incremental/incremental_results_20251109_225429_batch001_batch_001.json\n",
      "üìä Progress: 30/580 (5.2%)\n",
      "   Speed: 0.3 queries/sec | Success: 93.3%\n",
      "   ETA: 35m | Concurrency: 10 | Batch size: 30\n",
      "‚è≥ Standard delay: 0.5s (performance: 93.3%)\n",
      "üöÄ Processing batch 2/20 (30 queries, concurrency: 10, current batch size: 30)\n",
      "‚ö†Ô∏è  Empty/invalid response, retrying in 1.0s... (attempt 1/3)\n",
      "‚ö†Ô∏è  Empty/invalid response, retrying in 2.0s... (attempt 2/3)\n",
      "‚ùå Max retries reached, returning invalid response\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîÑ Progress is saved incrementally to logs/control/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müöÄ Starting control experiment...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m control_results = \u001b[43mrun_control_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODELS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONTROL_REPETITIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_open_router\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALL_OPEN_ROUTER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINITIAL_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINITIAL_CONCURRENCY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_CONCURRENCY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43madaptive_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mADAPTIVE_MODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONTROL_TAG\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Control experiment complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Results saved to: logs/control/\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Cyber/openai/whitehouse/consistencyAI/duplicity/control_experiment.py:139\u001b[39m, in \u001b[36mrun_control_experiment\u001b[39m\u001b[34m(models, repetitions, all_open_router, initial_batch_size, initial_concurrency, max_concurrency, adaptive_mode, max_retries, tag)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Run queries\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Querying models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m results = \u001b[43mquery_llm_fast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnested_queries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrol_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlist_of_models\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43madaptive_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43madaptive_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_open_router\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_open_router\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_100_percent_success\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_incremental\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubdir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontrol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    151\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m    154\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Control experiment complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Cyber/openai/whitehouse/consistencyAI/duplicity/fast_robust_queries.py:980\u001b[39m, in \u001b[36mquery_llm_fast\u001b[39m\u001b[34m(nested_queries, list_of_models, initial_batch_size, initial_concurrency, max_concurrency, adaptive_mode, all_open_router, max_retries, retry_delay, ensure_100_percent_success, save_incremental, incremental_interval, start_batch_number, initial_incremental_counter, subdir)\u001b[39m\n\u001b[32m    963\u001b[39m processor = FastRobustQueryProcessor(\n\u001b[32m    964\u001b[39m     initial_batch_size=initial_batch_size,\n\u001b[32m    965\u001b[39m     initial_concurrency=initial_concurrency,\n\u001b[32m   (...)\u001b[39m\u001b[32m    976\u001b[39m     subdir=subdir\n\u001b[32m    977\u001b[39m )\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_of_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_open_router\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    982\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Cyber/openai/whitehouse/consistencyAI/venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Cyber/openai/whitehouse/consistencyAI/venv/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Cyber/openai/whitehouse/consistencyAI/venv/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.11/selectors.py:561\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    559\u001b[39m ready = []\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m     kev_list = \u001b[38;5;28mself\u001b[39m._selector.control(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTROL EXPERIMENT: Measuring Within-Model Variance\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚ö†Ô∏è  This will run the same query 10 times for each of the 30 models\")\n",
    "print(\"   Total queries: 300\")\n",
    "print(\"   Estimated time: ~30 minutes\")\n",
    "print(\"\\nüîÑ Progress is saved incrementally to logs/control/\")\n",
    "print(\"\\nüöÄ Starting control experiment...\\n\")\n",
    "\n",
    "control_results = run_control_experiment(\n",
    "    models=MODELS,\n",
    "    repetitions=CONTROL_REPETITIONS,\n",
    "    all_open_router=ALL_OPEN_ROUTER,\n",
    "    initial_batch_size=INITIAL_BATCH_SIZE,\n",
    "    initial_concurrency=INITIAL_CONCURRENCY,\n",
    "    max_concurrency=MAX_CONCURRENCY,\n",
    "    adaptive_mode=ADAPTIVE_MODE,\n",
    "    max_retries=5,\n",
    "    tag=CONTROL_TAG\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Control experiment complete!\")\n",
    "print(f\"   Results saved to: logs/control/\")\n",
    "for model in control_results:\n",
    "    total_responses = sum(len(personas) for personas in control_results[model].values())\n",
    "    print(f\"   {model}: {total_responses} responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Control Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Computing control similarities...\")\n",
    "\n",
    "control_matrices, control_dfs, control_personas, control_embeddings = \\\n",
    "    supercompute_similarities(control_results)\n",
    "\n",
    "# Save similarities\n",
    "control_sim_path = save_similarity_results(\n",
    "    control_matrices, control_dfs, control_personas, control_embeddings,\n",
    "    tag=CONTROL_TAG, subdir=\"control\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Control similarities computed and saved!\")\n",
    "print(f\"   Matrices computed: {sum(len(topics) for topics in control_matrices.values())}\")\n",
    "print(f\"   Saved to: {control_sim_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT: Load Cached Control Results\n",
    "\n",
    "**Skip control experiment** and load cached data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load cached control results\n",
    "# # Uncomment to use cached data:\n",
    "\n",
    "# control_results = load_latest_fast_results(subdir=\"control\")\n",
    "# if control_results:\n",
    "#     print(\"‚úÖ Loaded control results from cache!\")\n",
    "#     for model in control_results:\n",
    "#         total_responses = sum(len(personas) for personas in control_results[model].values())\n",
    "#         print(f\"   {model}: {total_responses} responses\")\n",
    "    \n",
    "#     # Load similarities\n",
    "#     control_sim_data = load_latest_similarity_results(subdir=\"control\")\n",
    "#     if control_sim_data:\n",
    "#         control_matrices, control_dfs, control_personas, control_embeddings = control_sim_data\n",
    "#         print(f\"\\n‚úÖ Loaded control similarities from cache!\")\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è No cached control similarities found. Run the cell above to compute.\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è No cached control results found. Run the control experiment above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: MAIN EXPERIMENT\n",
    "\n",
    "## Measure Across-Persona Variance\n",
    "\n",
    "Query 100 diverse personas across 15 topics with 30 models.\n",
    "\n",
    "**Estimated time:** 6-12 hours for 45,000 queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MAIN EXPERIMENT: Measuring Across-Persona Variance\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìã Fetching {NUM_PERSONAS} personas from NVIDIA Nemotron dataset...\\n\")\n",
    "\n",
    "main_personas = get_and_clean_personas(\n",
    "    offset=0,\n",
    "    length=NUM_PERSONAS,\n",
    "    cache=True,\n",
    "    tag=MAIN_TAG,\n",
    "    subdir=\"main\"\n",
    ")\n",
    "\n",
    "num_personas = len(main_personas.get('rows', []))\n",
    "print(f\"\\n‚úÖ Loaded {num_personas} personas\")\n",
    "print(f\"   Saved to: logs/main/\")\n",
    "\n",
    "# Show sample\n",
    "if main_personas['rows']:\n",
    "    sample = main_personas['rows'][0]['row']\n",
    "    print(f\"\\nSample Persona:\")\n",
    "    print(f\"   Age: {sample.get('age')}, Sex: {sample.get('sex')}\")\n",
    "    print(f\"   Location: {sample.get('city')}, {sample.get('state')}\")\n",
    "    print(f\"   Occupation: {sample.get('occupation')}\")\n",
    "    print(f\"   Persona: {sample.get('persona', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT: Load Cached Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load cached personas\n",
    "# # Uncomment to use cached data:\n",
    "\n",
    "# main_personas = load_latest_personas(subdir=\"main\")\n",
    "# if main_personas:\n",
    "#     num_personas = len(main_personas.get('rows', []))\n",
    "#     print(f\"‚úÖ Loaded {num_personas} personas from cache\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è No cached personas found. Run the cell above to fetch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüîÑ Generating queries for {len(TOPICS)} topics...\\n\")\n",
    "\n",
    "main_queries = generate_queries_for_personas(main_personas, TOPICS)\n",
    "\n",
    "total_queries_per_model = sum(len(topic_queries) for topic_queries in main_queries.values())\n",
    "print(f\"‚úÖ Generated {total_queries_per_model:,} queries per model\")\n",
    "print(f\"   Total across all {len(MODELS)} models: {total_queries_per_model * len(MODELS):,}\")\n",
    "print(f\"\\n   Breakdown:\")\n",
    "for topic in list(main_queries.keys())[:3]:  # Show first 3 topics\n",
    "    print(f\"   - {topic}: {len(main_queries[topic])} queries\")\n",
    "if len(main_queries) > 3:\n",
    "    print(f\"   ... and {len(main_queries) - 3} more topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Query LLMs\n",
    "\n",
    "**‚ö†Ô∏è WARNING: This will take 6-12 hours and make ~45,000 API calls!**\n",
    "\n",
    "Progress is saved incrementally to `logs/main/incremental/` - you can stop and resume anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERYING LLMs - MAIN EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚ö†Ô∏è  This will make {total_queries_per_model * len(MODELS):,} API calls\")\n",
    "print(f\"   Estimated time: ~{total_queries_per_model * len(MODELS) / 10 / 60:.1f} minutes ({total_queries_per_model * len(MODELS) / 10 / 3600:.1f} hours)\")\n",
    "print(f\"\\nüîÑ Progress saved incrementally to: logs/main/incremental/\")\n",
    "print(\"   You can stop and resume anytime using query_llm_fast_resume()\")\n",
    "print(\"\\nüöÄ Starting main experiment queries...\\n\")\n",
    "\n",
    "main_results = query_llm_fast(\n",
    "    nested_queries=main_queries,\n",
    "    list_of_models=MODELS,\n",
    "    initial_batch_size=INITIAL_BATCH_SIZE,\n",
    "    initial_concurrency=INITIAL_CONCURRENCY,\n",
    "    max_concurrency=MAX_CONCURRENCY,\n",
    "    adaptive_mode=ADAPTIVE_MODE,\n",
    "    all_open_router=ALL_OPEN_ROUTER,\n",
    "    max_retries=5,\n",
    "    ensure_100_percent_success=True,\n",
    "    save_incremental=True,\n",
    "    subdir=\"main\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Main experiment complete!\")\n",
    "print(f\"   Results saved to: logs/main/\")\n",
    "for model in list(main_results.keys())[:5]:  # Show first 5 models\n",
    "    total_responses = sum(len(personas) for personas in main_results[model].values())\n",
    "    print(f\"   {model}: {total_responses} responses\")\n",
    "if len(main_results) > 5:\n",
    "    print(f\"   ... and {len(main_results) - 5} more models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESUME: Continue Interrupted Main Experiment\n",
    "\n",
    "**Use this cell if the main experiment was interrupted and you need to resume.**\n",
    "\n",
    "This will:\n",
    "- Load progress from the most recent incremental save\n",
    "- Continue querying from where it left off\n",
    "- Ensure 100% completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RESUME: Uncomment and run this cell if you need to resume an interrupted experiment\n",
    "# # (Instead of running the main query cell above)\n",
    "\n",
    "# from duplicity import load_incremental_results\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "\n",
    "# # Find the most recent incremental save file\n",
    "# incremental_dir = \"logs/main/incremental\"\n",
    "# incremental_files = sorted(\n",
    "#     Path(incremental_dir).glob(\"incremental_*.json\"),\n",
    "#     key=lambda p: p.stat().st_mtime,\n",
    "#     reverse=True\n",
    "# )\n",
    "\n",
    "# if not incremental_files:\n",
    "#     print(\"‚ö†Ô∏è  No incremental save files found in logs/main/incremental/\")\n",
    "#     print(\"   Run the main query cell above to start the experiment.\")\n",
    "# else:\n",
    "#     latest_incremental = str(incremental_files[0])\n",
    "#     print(f\"üìÇ Found incremental save: {latest_incremental}\")\n",
    "#     \n",
    "#     # Load partial results to see progress\n",
    "#     partial_results = load_incremental_results(latest_incremental)\n",
    "#     total_expected = len(MODELS) * total_queries_per_model\n",
    "#     completed = sum(\n",
    "#         len(personas) \n",
    "#         for model_data in partial_results.values() \n",
    "#         for personas in model_data.values()\n",
    "#     )\n",
    "#     print(f\"üìä Progress: {completed}/{total_expected} queries complete ({100*completed/total_expected:.1f}%)\")\n",
    "#     print(f\"\\nüîÑ Resuming from: {latest_incremental}\\n\")\n",
    "#     \n",
    "#     # Resume the experiment\n",
    "#     main_results = query_llm_fast_resume(\n",
    "#         incremental_file_path=latest_incremental,\n",
    "#         original_queries=main_queries,\n",
    "#         list_of_models=MODELS,\n",
    "#         initial_batch_size=INITIAL_BATCH_SIZE,\n",
    "#         initial_concurrency=INITIAL_CONCURRENCY,\n",
    "#         max_concurrency=MAX_CONCURRENCY,\n",
    "#         adaptive_mode=ADAPTIVE_MODE,\n",
    "#         all_open_router=ALL_OPEN_ROUTER,\n",
    "#         max_retries=5,\n",
    "#         ensure_100_percent_success=True,\n",
    "#         save_incremental=True,\n",
    "#         subdir=\"main\"\n",
    "#     )\n",
    "#     \n",
    "#     print(\"\\n‚úÖ Resumed experiment complete!\")\n",
    "#     print(f\"   Results saved to: logs/main/\")\n",
    "#     for model in list(main_results.keys())[:5]:\n",
    "#         total_responses = sum(len(personas) for personas in main_results[model].values())\n",
    "#         print(f\"   {model}: {total_responses} responses\")\n",
    "#     if len(main_results) > 5:\n",
    "#         print(f\"   ... and {len(main_results) - 5} more models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT: Load Cached Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load cached main results\n",
    "# # Uncomment to use cached data:\n",
    "\n",
    "# main_results = load_latest_fast_results(subdir=\"main\")\n",
    "# if main_results:\n",
    "#     print(\"‚úÖ Loaded main results from cache!\")\n",
    "#     for model in list(main_results.keys())[:5]:\n",
    "#         total_responses = sum(len(personas) for personas in main_results[model].values())\n",
    "#         print(f\"   {model}: {total_responses} responses\")\n",
    "#     if len(main_results) > 5:\n",
    "#         print(f\"   ... and {len(main_results) - 5} more models\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è No cached main results found. Run the query cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute Main Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Computing main experiment similarities...\")\n",
    "print(\"   This will compute embeddings and similarity matrices for all model/topic combinations\")\n",
    "print(\"   Estimated time: 5-10 minutes\\n\")\n",
    "\n",
    "main_matrices, main_dfs, main_personas_ids, main_embeddings = \\\n",
    "    supercompute_similarities(main_results)\n",
    "\n",
    "# Save similarities\n",
    "main_sim_path = save_similarity_results(\n",
    "    main_matrices, main_dfs, main_personas_ids, main_embeddings,\n",
    "    tag=MAIN_TAG, subdir=\"main\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Main similarities computed and saved!\")\n",
    "print(f\"   Matrices computed: {sum(len(topics) for topics in main_matrices.values())}\")\n",
    "print(f\"   Saved to: {main_sim_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT: Load Cached Main Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Load cached main similarities\n",
    "# # Uncomment to use cached data:\n",
    "\n",
    "# main_sim_data = load_latest_similarity_results(subdir=\"main\")\n",
    "# if main_sim_data:\n",
    "#     main_matrices, main_dfs, main_personas_ids, main_embeddings = main_sim_data\n",
    "#     print(\"‚úÖ Loaded main similarities from cache!\")\n",
    "#     print(f\"   Matrices computed: {sum(len(topics) for topics in main_matrices.values())}\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è No cached main similarities found. Run the cell above to compute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: VARIANCE ANALYSIS\n",
    "\n",
    "## Compare Control vs. Persona Variance\n",
    "\n",
    "Analyze how models vary internally (control) vs. across personas (main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"output/variance_comparison\", exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE ANALYSIS: Control vs. Persona Variance\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìà Computing within-model variance (control)...\")\n",
    "\n",
    "control_variance = compute_within_model_variance(\n",
    "    control_results,\n",
    "    control_matrices\n",
    ")\n",
    "\n",
    "control_variance.to_csv(\"output/variance_comparison/control_variance.csv\", index=False)\n",
    "print(f\"   Mean control similarity: {control_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Std dev: {control_variance['mean_similarity'].std():.4f}\")\n",
    "print(f\"   Saved to: output/variance_comparison/control_variance.csv\")\n",
    "\n",
    "print(\"\\nüìâ Computing across-persona variance (main)...\")\n",
    "\n",
    "persona_variance = compute_across_persona_variance(\n",
    "    main_results,\n",
    "    main_matrices\n",
    ")\n",
    "\n",
    "persona_variance.to_csv(\"output/variance_comparison/persona_variance.csv\", index=False)\n",
    "print(f\"   Mean persona similarity: {persona_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Std dev: {persona_variance['mean_similarity'].std():.4f}\")\n",
    "print(f\"   Saved to: output/variance_comparison/persona_variance.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ Variance computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating variance comparison visualizations...\\n\")\n",
    "\n",
    "create_variance_comparison_visualizations(\n",
    "    control_variance,\n",
    "    persona_variance,\n",
    "    output_dir=\"output/variance_comparison\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations created!\")\n",
    "print(\"   Output files:\")\n",
    "print(\"   - comparison_bar_chart.png\")\n",
    "print(\"   - comparison_scatter.png\")\n",
    "print(\"   - comparison_heatmap.png\")\n",
    "print(\"   - variance_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Variance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìù Generating variance analysis report...\\n\")\n",
    "\n",
    "report = generate_variance_report(\n",
    "    control_variance,\n",
    "    persona_variance,\n",
    "    output_path=\"output/variance_comparison/report.txt\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìÅ All variance analysis files saved to: output/variance_comparison/\")\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "print(f\"   Control mean similarity: {control_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Persona mean similarity: {persona_variance['mean_similarity'].mean():.4f}\")\n",
    "print(f\"   Difference: {abs(control_variance['mean_similarity'].mean() - persona_variance['mean_similarity'].mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 4: STANDARD ANALYSES\n",
    "\n",
    "## Traditional ConsistencyAI visualizations and analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average scores from main experiment\n",
    "avg_scores = collect_avg_scores_by_model(main_matrices)\n",
    "\n",
    "print(\"\\nAverage Similarity Scores (Main Experiment):\")\n",
    "print(\"   (Higher = more consistent across personas)\\n\")\n",
    "\n",
    "for model in list(avg_scores.keys())[:5]:  # Show first 5\n",
    "    print(f\"\\n{model}:\")\n",
    "    for topic in list(avg_scores[model].keys())[:3]:  # Show first 3 topics\n",
    "        score = avg_scores[model][topic]\n",
    "        print(f\"   {topic}: {score:.4f}\")\n",
    "    if len(avg_scores[model]) > 3:\n",
    "        print(f\"   ... and {len(avg_scores[model]) - 3} more topics\")\n",
    "    overall = np.mean(list(avg_scores[model].values()))\n",
    "    print(f\"   Overall: {overall:.4f}\")\n",
    "\n",
    "if len(avg_scores) > 5:\n",
    "    print(f\"\\n... and {len(avg_scores) - 5} more models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Overall Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating overall leaderboard...\\n\")\n",
    "\n",
    "plot_overall_leaderboard(avg_scores, save_path=\"output\", show=True)\n",
    "\n",
    "print(\"\\n‚úÖ Leaderboard saved to: output/overall_leaderboard.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Topic-Specific Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating topic-specific plots...\\n\")\n",
    "\n",
    "plot_similarity_by_sphere(avg_scores, save_path=\"output\")\n",
    "\n",
    "print(\"\\n‚úÖ Topic plots saved to: output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Similarity Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Creating similarity heatmaps...\")\n",
    "print(\"   (This creates one heatmap per model/topic combination)\\n\")\n",
    "\n",
    "heatmap_count = 0\n",
    "for model in main_matrices:\n",
    "    for topic in main_matrices[model]:\n",
    "        safe_model = model.replace(\"/\", \"_\")\n",
    "        safe_topic = topic.replace(\" \", \"_\").replace(\"?\", \"\").replace(\"'\", \"\")\n",
    "        save_path = f\"output/heatmap_{safe_model}_{safe_topic}.png\"\n",
    "        \n",
    "        fig, ax = plot_similarity_matrix_with_values(\n",
    "            similarity_matrix=main_matrices[model][topic],\n",
    "            persona_ids=main_personas_ids[model][topic],\n",
    "            show_values=(len(main_personas_ids[model][topic]) <= 20),\n",
    "            model_name=model,\n",
    "            topic=topic,\n",
    "            save_path=save_path\n",
    "        )\n",
    "        plt.close(fig)\n",
    "        heatmap_count += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Created {heatmap_count} heatmaps in: output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Performing clustering analysis...\\n\")\n",
    "\n",
    "analysis_results = analyze_and_cluster_embeddings(\n",
    "    all_embeddings=main_embeddings,\n",
    "    all_similarity_matrices=main_matrices,\n",
    "    all_sorted_personas=main_personas_ids,\n",
    "    max_clusters=10,\n",
    "    random_state=42,\n",
    "    save_plots=True,\n",
    "    plots_dir=\"output/clustering\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Clustering analysis complete!\\n\")\n",
    "\n",
    "# Print summary\n",
    "print_analysis_summary(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Running central analysis...\\n\")\n",
    "\n",
    "per_model_per_topic, model_overall_weighted, topic_across_models_weighted, benchmark = \\\n",
    "    compute_central_analysis(main_matrices, output_dir=\"output/analysis\")\n",
    "\n",
    "print(\"\\n‚úÖ Central analysis complete!\\n\")\n",
    "\n",
    "# Display results\n",
    "print_central_analysis_summary(model_overall_weighted, topic_across_models_weighted, benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EXPERIMENT COMPLETE!\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "### What you just did:\n",
    "\n",
    "**Control Experiment:**\n",
    "- Measured within-model variance using Mary Alberti persona\n",
    "- Ran same query 10 times per model (300 total queries)\n",
    "- Identified most internally consistent models\n",
    "\n",
    "**Main Experiment:**\n",
    "- Loaded 100 diverse personas from NVIDIA Nemotron\n",
    "- Generated personalized queries across 15 sensitive topics\n",
    "- Queried 30 LLMs (45,000 total queries)\n",
    "- Measured across-persona variance\n",
    "\n",
    "**Variance Analysis:**\n",
    "- Compared control vs. persona variance\n",
    "- Identified most persona-sensitive models\n",
    "- Generated comprehensive visualizations and reports\n",
    "\n",
    "**Standard Analysis:**\n",
    "- Created similarity heatmaps\n",
    "- Generated leaderboards and topic plots\n",
    "- Performed clustering analysis\n",
    "- Computed central analysis metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Output Files\n",
    "\n",
    "### Control Experiment (`logs/control/`):\n",
    "- Query results and similarities\n",
    "- Incremental progress files\n",
    "\n",
    "### Main Experiment (`logs/main/`):\n",
    "- Persona data (100 personas)\n",
    "- Query results (45,000 responses)\n",
    "- Similarity matrices and embeddings\n",
    "- Incremental progress files\n",
    "\n",
    "### Variance Analysis (`output/variance_comparison/`):\n",
    "- `control_variance.csv` - Within-model variance stats\n",
    "- `persona_variance.csv` - Across-persona variance stats\n",
    "- `comparison_bar_chart.png` - Side-by-side comparison\n",
    "- `comparison_scatter.png` - Control vs. persona scatter\n",
    "- `comparison_heatmap.png` - Metrics heatmap\n",
    "- `variance_distributions.png` - Distribution box plots\n",
    "- `report.txt` - Detailed text report\n",
    "\n",
    "### Standard Analysis (`output/`):\n",
    "- Similarity heatmaps (one per model/topic)\n",
    "- Overall leaderboard\n",
    "- Topic-specific plots\n",
    "- Clustering visualizations (`output/clustering/`)\n",
    "- Central analysis CSVs (`output/analysis/`)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "**Understanding Variance:**\n",
    "- **High control variance** = Model is noisy/inconsistent with itself\n",
    "- **Low control variance** = Model is stable and consistent\n",
    "- **High persona variance** = Model adapts responses to different personas\n",
    "- **Low persona variance** = Model gives similar responses regardless of persona\n",
    "\n",
    "**Ideal Model Profile:**\n",
    "- ‚úÖ High control similarity (consistent with itself)\n",
    "- ‚úÖ Lower persona similarity (persona-aware)\n",
    "- = Stable behavior that adapts to demographic differences\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review variance comparison visualizations in `output/variance_comparison/`\n",
    "2. Read the detailed report: `output/variance_comparison/report.txt`\n",
    "3. Examine model-specific heatmaps in `output/`\n",
    "4. Analyze central analysis results in `output/analysis/`\n",
    "5. Share your findings!\n",
    "\n",
    "---\n",
    "\n",
    "**Built by the Duke Phishermen**  \n",
    "Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute\n",
    "\n",
    "With inquiries, questions, & feedback, please contact: peter dot banyas at duke dot edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
